<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="UTF-8">

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Home - page 2 &middot; Bit of A Byte
    
  </title>

  
  <!-- canonical link -->
  <link rel="canonical" href="/page/2/index.html">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Dosis">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">

  <!-- Icons -->
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png"> -->
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-130197091-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>My name is Natalie and I spend most of my time taking apart things I have no business taking apart.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/archive/">Archive</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/projects/">Projects</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/site/">Site Information</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/tags/">Tags</a>
        
      
    

    <a class="sidebar-nav-item" href="https://github.com/zyradyl/zyradyl.github.io">Currently v0.6.0</a>
  </nav>

  <div class="sidebar-item">
    <p>
      <a href="https://creativecommons.org/publicdomain/zero/1.0/">
        Ø 2018 - No Rights Reserved.
      </a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            
                
            <small><a href="http://127.0.0.1">$USER</a>@<a href="/" title="Home">bit-of-a-byte</a>
         ~ $ cat $(find /var/log -name '*.log' -print | awk 'NR>5&amp;&amp;NR&lt;=10')</small>
            
            </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2018/11/10/Yet-Another-Backup-System-Part-1/">
        Yet Another Backup System - Part 1
      </a>
    </h1>

    <span class="post-date">10 Nov 2018</span>

    <h2 id="introduction">Introduction</h2>
<p>I’ve been wanting to get back into blogging, or at least writing more
consistently, and I’ve also been looking for projects to work on. One of my
favorite things to do, oddly enough, is to work on systems that will manage
large amounts of data. So, I’ve decided to start a new project.</p>

<p>This is the first in a series of posts detailing the creation of a multi-tiered
backup system for me, some of my friends, and potentially (but not likely)
others as well. I will warn people ahead of time that I’m not building any
hardware that could be called “resilient,” instead I’m using things that I
already have around the house, plus a few extra cheap purchases, and a lot
of software to make things work.</p>

<p>Speaking of software, almost everything used will be cross-platform and open
source. I will likely be writing a lot of “glue,” in the form of shell scripts
or potentially even some ruby. I will also be trying to focus on keeping things
secure on untrusted servers, and secure in transit, to sometimes ridiculous
levels. That being said, this is <strong>NOT</strong> a zero-knowledge system. Since it is
for me, family, and friends, they are made aware of this ahead of time. In time,
it is possible that this setup will grow and evolve to make a zero-knowledge
system that functions similarly to this initial design idea, but that’s far
down the line.</p>

<p>None of these blog posts will likely be as refined as my Icinga2 tutorials. One
of the things I am trying to do is just get myself into the habit of documenting
my projects. If I keep having to revise, edit, and source, I’ll eventually give
up. So if there is anything you are interested in learning more about, Google
will likely be your best friend.</p>

<p>Essentially, these are meant to explain how I built the system, and why I did
what I did. Sometimes that may be as simple as “because I like them more” and
other times it may have to do with more in depth research.</p>

<h2 id="environment">Environment</h2>

<p>The server will exist in an environment with a battery backup system provided
for the server, any data drives, and the core router that the server will be
connected to. Connection to the internet is provided with a 100mbps symmetrical
fiber connection that can be upgraded instantly to 1Gbps symmetrical if needed.
Connections between the server and the fiber node is provided via CAT-7A
cabling. Routing is handled by a Ubiquiti EdgeRouter Lite, with 3gbps line rate
and one million packet per second routing. IPv6 and IPv4 are supported.</p>

<p>Eventually the router will be able to provide routes both to the public internet
and to the dn42 darknet and Tor services.</p>

<h2 id="hardware">Hardware</h2>

<p>The hardware isn’t exactly great, but it is the best that can be done at this
time. The server consists of a Mac Mini Model 2,1 made circa early 2007. It
contains a dual core Intel Core2 Duo processor running at 2.0 GHz.
<code class="highlighter-rouge">/proc/cpuinfo</code> is provided here:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processor</span>       <span class="p">:</span> <span class="m">0</span>
<span class="n">vendor_id</span>       <span class="p">:</span> <span class="n">GenuineIntel</span>
<span class="n">cpu</span> <span class="n">family</span>      <span class="p">:</span> <span class="m">6</span>
<span class="k">model</span>           <span class="p">:</span> <span class="m">15</span>
<span class="k">model</span> <span class="n">name</span>      <span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span><span class="m">2</span> <span class="n">CPU</span>         <span class="n">T7200</span>  <span class="p">@</span> <span class="m">2.00</span><span class="n">GHz</span>
<span class="n">stepping</span>        <span class="p">:</span> <span class="m">6</span>
<span class="n">microcode</span>       <span class="p">:</span> <span class="m">0xd1</span>
<span class="n">cpu</span> <span class="n">MHz</span>         <span class="p">:</span> <span class="m">2000.000</span>
<span class="n">cache</span> <span class="n">size</span>      <span class="p">:</span> <span class="m">4096</span> <span class="n">KB</span>
<span class="n">physical</span> <span class="n">id</span>     <span class="p">:</span> <span class="m">0</span>
<span class="n">siblings</span>        <span class="p">:</span> <span class="m">2</span>
<span class="n">core</span> <span class="n">id</span>         <span class="p">:</span> <span class="m">0</span>
<span class="n">cpu</span> <span class="n">cores</span>       <span class="p">:</span> <span class="m">2</span>
<span class="n">apicid</span>          <span class="p">:</span> <span class="m">0</span>
<span class="n">initial</span> <span class="n">apicid</span>  <span class="p">:</span> <span class="m">0</span>
<span class="n">fpu</span>             <span class="p">:</span> <span class="n">yes</span>
<span class="n">fpu_exception</span>   <span class="p">:</span> <span class="n">yes</span>
<span class="n">cpuid</span> <span class="n">level</span>     <span class="p">:</span> <span class="m">10</span>
<span class="n">wp</span>              <span class="p">:</span> <span class="n">yes</span>
<span class="n">flags</span>           <span class="p">:</span> <span class="n">fpu</span> <span class="n">vme</span> <span class="n">de</span> <span class="n">pse</span> <span class="n">tsc</span> <span class="n">msr</span> <span class="n">pae</span> <span class="n">mce</span> <span class="n">cx8</span> <span class="n">apic</span> <span class="n">sep</span> <span class="n">mtrr</span> <span class="n">pge</span> <span class="n">mca</span> <span class="n">cmov</span> <span class="n">pat</span> <span class="n">pse36</span> <span class="n">clflush</span> <span class="n">dts</span> <span class="n">acpi</span> <span class="n">mmx</span> <span class="n">fxsr</span> <span class="n">sse</span> <span class="n">sse2</span> <span class="n">ss</span> <span class="n">ht</span> <span class="n">tm</span> <span class="n">pbe</span> <span class="n">syscall</span> <span class="n">nx</span> <span class="n">lm</span> <span class="n">constant_tsc</span> <span class="n">arch_perfmon</span>
<span class="n">pebs</span> <span class="n">bts</span> <span class="n">rep_good</span> <span class="n">nopl</span> <span class="n">cpuid</span> <span class="n">aperfmperf</span> <span class="n">pni</span> <span class="n">dtes64</span> <span class="n">monitor</span> <span class="n">ds_cpl</span> <span class="n">vmx</span> <span class="n">est</span> <span class="n">tm2</span> <span class="n">ssse3</span> <span class="n">cx16</span> <span class="n">xtpr</span> <span class="n">pdcm</span> <span class="n">lahf_lm</span> <span class="n">pti</span> <span class="n">tpr_shadow</span> <span class="n">dtherm</span>
<span class="n">bugs</span>            <span class="p">:</span> <span class="n">cpu_meltdown</span> <span class="n">spectre_v1</span> <span class="n">spectre_v2</span> <span class="n">spec_store_bypass</span> <span class="n">l1tf</span>
<span class="n">bogomips</span>        <span class="p">:</span> <span class="m">3999.68</span>
<span class="n">clflush</span> <span class="n">size</span>    <span class="p">:</span> <span class="m">64</span>
<span class="n">cache_alignment</span> <span class="p">:</span> <span class="m">64</span>
<span class="n">address</span> <span class="n">sizes</span>   <span class="p">:</span> <span class="m">36</span> <span class="n">bits</span> <span class="n">physical</span><span class="p">,</span> <span class="m">48</span> <span class="n">bits</span> <span class="n">virtual</span>
<span class="n">power</span> <span class="n">management</span><span class="p">:</span>

<span class="n">processor</span>       <span class="p">:</span> <span class="m">1</span>
<span class="n">vendor_id</span>       <span class="p">:</span> <span class="n">GenuineIntel</span>
<span class="n">cpu</span> <span class="n">family</span>      <span class="p">:</span> <span class="m">6</span>
<span class="k">model</span>           <span class="p">:</span> <span class="m">15</span>
<span class="k">model</span> <span class="n">name</span>      <span class="p">:</span> <span class="n">Intel</span><span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Core</span><span class="p">(</span><span class="n">TM</span><span class="p">)</span><span class="m">2</span> <span class="n">CPU</span>         <span class="n">T7200</span>  <span class="p">@</span> <span class="m">2.00</span><span class="n">GHz</span>
<span class="n">stepping</span>        <span class="p">:</span> <span class="m">6</span>
<span class="n">microcode</span>       <span class="p">:</span> <span class="m">0xd1</span>
<span class="n">cpu</span> <span class="n">MHz</span>         <span class="p">:</span> <span class="m">1000.000</span>
<span class="n">cache</span> <span class="n">size</span>      <span class="p">:</span> <span class="m">4096</span> <span class="n">KB</span>
<span class="n">physical</span> <span class="n">id</span>     <span class="p">:</span> <span class="m">0</span>
<span class="n">siblings</span>        <span class="p">:</span> <span class="m">2</span>
<span class="n">core</span> <span class="n">id</span>         <span class="p">:</span> <span class="m">1</span>
<span class="n">cpu</span> <span class="n">cores</span>       <span class="p">:</span> <span class="m">2</span>
<span class="n">apicid</span>          <span class="p">:</span> <span class="m">1</span>
<span class="n">initial</span> <span class="n">apicid</span>  <span class="p">:</span> <span class="m">1</span>
<span class="n">fpu</span>             <span class="p">:</span> <span class="n">yes</span>
<span class="n">fpu_exception</span>   <span class="p">:</span> <span class="n">yes</span>
<span class="n">cpuid</span> <span class="n">level</span>     <span class="p">:</span> <span class="m">10</span>
<span class="n">wp</span>              <span class="p">:</span> <span class="n">yes</span>
<span class="n">flags</span>           <span class="p">:</span> <span class="n">fpu</span> <span class="n">vme</span> <span class="n">de</span> <span class="n">pse</span> <span class="n">tsc</span> <span class="n">msr</span> <span class="n">pae</span> <span class="n">mce</span> <span class="n">cx8</span> <span class="n">apic</span> <span class="n">sep</span> <span class="n">mtrr</span> <span class="n">pge</span> <span class="n">mca</span> <span class="n">cmov</span> <span class="n">pat</span> <span class="n">pse36</span> <span class="n">clflush</span> <span class="n">dts</span> <span class="n">acpi</span> <span class="n">mmx</span> <span class="n">fxsr</span> <span class="n">sse</span> <span class="n">sse2</span> <span class="n">ss</span> <span class="n">ht</span> <span class="n">tm</span> <span class="n">pbe</span> <span class="n">syscall</span> <span class="n">nx</span> <span class="n">lm</span> <span class="n">constant_tsc</span> <span class="n">arch_perfmon</span>
<span class="n">pebs</span> <span class="n">bts</span> <span class="n">rep_good</span> <span class="n">nopl</span> <span class="n">cpuid</span> <span class="n">aperfmperf</span> <span class="n">pni</span> <span class="n">dtes64</span> <span class="n">monitor</span> <span class="n">ds_cpl</span> <span class="n">vmx</span> <span class="n">est</span> <span class="n">tm2</span> <span class="n">ssse3</span> <span class="n">cx16</span> <span class="n">xtpr</span> <span class="n">pdcm</span> <span class="n">lahf_lm</span> <span class="n">pti</span> <span class="n">tpr_shadow</span> <span class="n">dtherm</span>
<span class="n">bugs</span>            <span class="p">:</span> <span class="n">cpu_meltdown</span> <span class="n">spectre_v1</span> <span class="n">spectre_v2</span> <span class="n">spec_store_bypass</span> <span class="n">l1tf</span>
<span class="n">bogomips</span>        <span class="p">:</span> <span class="m">3999.68</span>
<span class="n">clflush</span> <span class="n">size</span>    <span class="p">:</span> <span class="m">64</span>
<span class="n">cache_alignment</span> <span class="p">:</span> <span class="m">64</span>
<span class="n">address</span> <span class="n">sizes</span>   <span class="p">:</span> <span class="m">36</span> <span class="n">bits</span> <span class="n">physical</span><span class="p">,</span> <span class="m">48</span> <span class="n">bits</span> <span class="n">virtual</span>
<span class="n">power</span> <span class="n">management</span><span class="p">:</span>
</code></pre></div></div>

<p>The server has been modified substansially from it’s factory state. The sound
card has been removed, as has the WiFi card and Bluetooth card. IR functionality
has been disconnected. Any extraneous wires have been removed. The original
160GB Hitachi hard drive has been replaced with a 240GB solid state drive from
OWC. The ram has been maxed out with 4GBs, again provided from OWC. The system
is not capable of addressing all 4GB despite being a 64-bit system due to
limitations imposed in the EFI. The CMOS battery was replaced, and the processor
was unseated, cleaned, and thermal material replaced. The fan was also replaced
to head off any potential issues caused by old age. You can see a picture of the
internals of the system below.</p>

<p><img src="/images/posts/mac-mini.jpeg" alt="Modified MacMini2,1" /></p>

<p>The system is connected to the router with a Fluke-Certified CAT-7A S/FTP dual
shielded cable. The interface it is connected to supports a 1Gbps full-duplex
connection.</p>

<p>The primary data disk is a G-Disk USB-C 4TB external drive. This drive utilizes
a white label Western Digital Red WD40EFRX. It is connected to the Mac Mini via
an Anker PowerLine+ USB-C to USB-A nylon cable.</p>

<p>The older Mac Mini’s would refuse to boot if the system could not detect that a
monitor was connected to the system. Apple claims that this is because the Mac
Mini was intended to be a true personal computer, which would generally prohibit
the system’s operation without a display device. Strangely, no such detection is
present to test for either a keyboard or a mouse.</p>

<p>To bypass this issue, a DVI Dummy Plug is installed in the Mini’s DVI port. This
plug uses EDID to tell the system that a 1920x1200 capable display is connected.
This has also been reported to be useful in accelerating VNC Remote
Administration, however no GUI is present on the system.</p>

<h2 id="operating-system">Operating System</h2>

<p>The operating system used on the server is OpenSUSE Leap 15 x64. You may
remember that I mentioned above that the EFI used on the system is 32-bits.
This limitation is bypassed by formatting the internal server drive to utilize
a legacy MBR partition table. From this, GRUB2 loads in legacy mode, which is
32-bits. This provides the shim that allows the 32-bit system firmware to load
a 64-bit operating system. Over 72 hours of stress testing revealed no
instability with the operating system booted via this method.</p>

<p>OpenSUSE was installed using the Server profile. Originally the
Transactional-Server profile was used, however the early morning reboots
could cause issues, and the installation did not seem to play well with LVM.</p>

<h2 id="hardware-stress-testing">Hardware Stress Testing</h2>

<p>The internal solid state drive was tested via an OpenSUSE Live distribution. The
computer was first suspended and resumed to bypass the system’s EFI’s proclivity
for freezing the internal drive’s security interface. Once the interface was
unlocked, <code class="highlighter-rouge">smartctl</code> was used to trigger the drives internal SMART short test,
conveyance test, and extended test, in that order. All results were nominal.
The internal drive was then wiped via <code class="highlighter-rouge">hdparm</code> by sending an ATA Enhanced Secure
Erase Command with a non-null password.</p>

<p>A quick installation of OpenSUSE Leap 15 was then performed.</p>

<p>The hardware was then tested from inside the new installation utilizing the
Stress-NG test suite for over 24 hours. During this time, four virtual memory
test workers iterated over 100% of the available memory to force swapping as
well as check for ram errors. Two CPU test workers were ran on the aggressive
settings, and an IO worker was deployed to check the SSD interface. All systems
passed the stress test perfectly, with benchmarks appropriate for a system from
2007.</p>

<p>The external drive was then connected to the system. A battery of tests were
performed. First, the drive was tested via <code class="highlighter-rouge">smartctl</code>. As the drive is connected
via USB, the <code class="highlighter-rouge">-d sat,16</code> option was passed to allow communication directly with
the USB-SATA bridge. The drive was tested out of the box using the SMART short
self test, conveyance test, and extended self test. This first round of testing
took 10 hours to execute. After the inital round of tests, the SMART attributes
table and its corresponding values was recorded.</p>

<p>Then the <code class="highlighter-rouge">badblocks</code> command was ran to write to and read from every sector on
the drive. The full command was
<code class="highlighter-rouge">badblocks -wsv -t random -b 4096 -c 4096 -p 4 -o /root/sdb.log /dev/sdb</code>. This
performed four read-write-verify passes. Each pass took approximately 50 hours
to complete.</p>

<p>Once the <code class="highlighter-rouge">badblocks</code> battery was complete, <code class="highlighter-rouge">smartctl</code> was once again used to
run the same testing sequence of short, conveyance, and extended self tests. The
attribute table was once again dumped, it’s values recorded, and the values then
compared to those from the table dumped prior to running <code class="highlighter-rouge">badblocks</code>.</p>

<p>As these tests are actually still in progress, the results are not yet
definitive.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This first post simply outlines the primary variables in this project. Hardware,
operating system, and environment should all be assumed to remain static for
the remaining project entries. Should anything change, I will likely update this
entry versus notating it in the respective entry where the change was made. It
will be made clear that this entry has been edited, should that come to pass.</p>

<p>Extensive testing was performed to detect any chance of infant-mortality in the
hardware. I have previously had negative experiences utilizing hardware right
out of the box.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2016/10/14/Collegi-Backup-Work-Notes/">
        Collegi Pixelmon - Developer Log
      </a>
    </h1>

    <span class="post-date">14 Oct 2016</span>

    <p>The following post is an extreme rough draft. In fact, it isn’t even actually a
post. These are my development notes from my refactoring of the collegi data
infrastructure. As such, they’re arranged in no real sensible order besides
having been written chronologically. Additionally, these have not been
proofread, grammar checked, copyedited, or spell checked, as i write them in an
IDE and not an actual text editor. As such, please don’t judge my writing
ability off of them. More importantly, however, these do not have the
standardized links that i provide to new concepts or commands in my blog posts,
as embedding links to things I already know or have access to in a developer log
that on average no one else sees just seems silly.</p>

<p>So, if you have questions, use google, and expect these to be updated over time.</p>

<p>The logs as of this posting run from 10/13/2016 to 10/16/2016, so over three
days of work. There is a -LOT- more to be done.</p>

<p>They are broken down into the following format. Each list is a set of specific
actions I took, and sometimes the list ends up with notes in it because, again,
no one generally sees these, but under the task list is the space reserved for
notes on the above list. Then a new task list is declared, then notes, then
tasks, and so on and so forth. Generally each new task heading would signify
a new blog post, talking about the tasks and the notes, so keep that in mind.</p>

<p>These were requested by Kan, a player on our server. Enjoy!</p>

<h3 id="tasks">Tasks</h3>

<ul>
  <li>Made a backup of the repository as it stood on 2016-10-13 in the event
anything breaks too badly during this.</li>
  <li>Removed all existing submodules from the <code class="highlighter-rouge">git</code> repository. Committed the
removal.</li>
  <li>Ran the previous backup script to make sure that 10/13 was backed up. This
included new additions to git annex.</li>
  <li>Forced git annex to drop the old SHA256E key-value backend files that were
made obsolete by the conversion to SHA512E key-value backend.</li>
</ul>

<p><em><strong>Notes 1:</strong> During this time, and while watching the way the version 1.0
backup script ran, I noticed there is a significant performance penalty for
moving the location of the local mirror. Borg uses the entire path as the file
name, so any deviation in the path spec causes it to treat the files as brand
new. Note that this does not cause any issues with de-duplication, but the
process of adding these files causes a massive performance hit. This made me
start thinking about including the local mirror in the git annex so that as
long as the annex was kept in tact in regards to metadata, the paths would
remain the same as all additions to Borg would take place from the same root
directory.</em></p>

<p><em>The problem with this would be the fact that annex keeps everything as
symlinks. As such, I am looking into the unlock feature of version six
repositories.</em></p>

<p><em><strong>Notes 2:</strong> Dropping unused from a local area goes -much- faster than
dropping from remote. Who knew, right?</em> :tongue:</p>

<h3 id="tasks-1">Tasks</h3>

<ul>
  <li><code class="highlighter-rouge">git-Annex</code> drop completed, but Finder isn’t showing a reduction in used drive
space, but I think this is more an error on the side of finder than something
with git annex, as <code class="highlighter-rouge">du -h</code> showed the directory was down to the size it should
have been. Once I manage to get this finder thing figured out, I’ll move on to
the next part.</li>
  <li>Finder is taking too bloody long to figure its shit out, so I moved on to the
next step in cleaning up the repository. I’m rewriting the commit history
to completely remove files I don’t need from the actual <code class="highlighter-rouge">git</code> repo. In theory
this shouldn’t touch <code class="highlighter-rouge">git-annex</code> at all, but that remains to be seen.</li>
  <li>Ran BFG Repo Cleaner on the following directories and files:
    <ul>
      <li>collegi.web</li>
      <li>collegi.pack</li>
      <li>collegi.git</li>
      <li>.DS_Store</li>
      <li>.gitmodules</li>
      <li>collegi.logs (Just for a moment, and we made backups.)</li>
      <li>collegi.configs</li>
    </ul>
  </li>
  <li>Ran filter branch to purge any empty commits left after the above.</li>
  <li>Expired original ref-logs, repacked archive.</li>
</ul>

<p><em><strong>Notes 3:</strong> At this point we had gone from 230 commits to 102 commits. We were
also left with the original envisioning of what this repo would be, which was
a simple git annex to push files to Backblaze b2 from the Borg repository. Now
to verify that all of our data is still 100% ok.</em></p>

<h3 id="tasks-2">Tasks</h3>

<ul>
  <li>Ran <code class="highlighter-rouge">git fsck</code></li>
  <li>Ran <code class="highlighter-rouge">git annex fsck</code></li>
</ul>

<p><em><strong>Notes 4:</strong> Wow this is going to take a long fucking time. Who woulda thunk
it.</em></p>

<p><em><strong>Notes 5:</strong> So apparently the current version of <code class="highlighter-rouge">git-annex</code> is using the old
mixed hashing method, which is a format that “we would like to stop using”
according to the wiki. Might need to migrate. Need to figure out how.</em></p>

<p><em><strong>Notes 6:</strong> From the wiki: “Initial benchmarks suggest that going from xX/yY/KEY/OBJ to xX/yY/OBJ directories would improve speed 3x.” It’s worth
migrating.</em></p>

<h3 id="tasks-3">Tasks</h3>

<ul>
  <li>Run <code class="highlighter-rouge">git annex uninit</code></li>
  <li>Reading through the <code class="highlighter-rouge">git-annex-init</code> man page to see what else we should
change now since we’re already migrating. Post Uninit we’re going to have to
run a full borg data consistancy check.</li>
</ul>

<p><em><strong>Notes 7:</strong> Ugh. The document I found was actually an theoretical one, and
while it is true that <code class="highlighter-rouge">git-annex</code> does use the new hashing format in bare
repositories there is no actual way to move to the new one in a regular repo.
So I am running an <code class="highlighter-rouge">uninit</code> for basically no reason. The only good thing about
this that I can think of is that I will be able to reform the final <code class="highlighter-rouge">git-annex</code>
repo in a much saner fashion. The bad news is that I have lost the log files,
unless <code class="highlighter-rouge">git-annex</code> is going to bring those back for me. I am annoyed.</em></p>

<p><em><strong>Notes 8:</strong> Good news! I just remembered that I had made a <code class="highlighter-rouge">rsyn</code>ced backup
of the repository before I started fucking with it. So I didn’t actually lose
the log files, I just went ahead and pulled them out of the <code class="highlighter-rouge">git-annex</code>
backup.</em></p>

<h3 id="tasks-4">Tasks</h3>
<ul>
  <li>After the git annex had uninitialized, I decided that if I was going to do
this whole damn thing over again I was going to do it right.</li>
  <li>Started a new <code class="highlighter-rouge">borg</code> repository in new-collegi. Pulled out contents from the
original <code class="highlighter-rouge">borg</code> repository, using backups to restore any files that got hit in
the above clusterfuck, then recompressed with maximum LZMA compression.</li>
  <li>During this period I also standardized how the <code class="highlighter-rouge">borg create</code> paths would work.
The server would exist within a collegi.mirror directory, and the entire
directory would be added to <code class="highlighter-rouge">borg</code> upon each run of the backup script. This
effectively means we never have to worry about the LZMA penalty discussed below
again after the first re-add, unless we do major server restructuring, because
paths will remain stable between commits.</li>
</ul>

<p><em><strong>Notes 9:</strong> The initial speed penalty for using LZMA is absolutely jaw
dropping. One <code class="highlighter-rouge">borg create</code> took eight hours to complete. Eight. However, I
quickly noticed that due to Borg’s de-duplication mechanism, the add times got
faster the more data I added, and gzip-9 to lzma-9 did actually yield some
improvement. It also reduces the incentive for me to do this fucking disaster
again, because of how much it absolutely fucking sucks.</em></p>

<p><em><strong>Notes 10:</strong> As an example of what I mean by the above, the initial adding of
1.8.9 took six hours with LZMA-9. When the map was changed from NewSeed over to
Collegi, it took another four hours just to update the paths and what not, even
though the data hadn’t updated, just the paths have changed. (This is indicated
by the fact that the total repository size barely increased, all the size that
changed could be explained by new metadata.) However, when the paths are kept
the same, adding 100GB of data takes 13 to 15 minutes. So, the benefit of
LZMA-9 is worth the initial startup, imho.</em></p>

<p><em><strong>Notes 11:</strong> <code class="highlighter-rouge">borg extract</code>ing from the GZIP-9 archives takes about 40
minutes, and that’s from highly de-duplicated and GZIP-9 archives. What this
means is that pulling from an lzma-9 is probably going to take about an hour,
depending on just how de-duplicated the archive is (as in, how many different
chunk files contain parts needed to reassemble the original content).</em></p>

<p><em><strong>Notes 12:</strong> Have hit the series of backups where things have moved into the
Users path, and I’m restructuring them. It made me think about how I will handle
the mirror directory in the future. I think I am going to do a few new things
with respect to the new setup. The mirror directory will be a part of the
<code class="highlighter-rouge">git-annex</code> repository, so there will be a new folder inside it called
<code class="highlighter-rouge">collegi.mirror</code> or something similar, and then I can move the new backup
script to be ran from the root directory, which will be beneficial. That way
everything is neatly packaged. the issue becomes mirroring this, because
uploading that much constantly changing data to backblaze would be literally
stupid, and not at all within our budget. What I will likely do is initialize
a “bare repository” on my time machine drive, and mirror the entirity of the
<code class="highlighter-rouge">git-annex</code> repository to that.</em></p>

<h3 id="mandatory-break-notes">Mandatory Break Notes</h3>

<ul>
  <li>You need to run borg info to make sure the latest creation thingy is the
proper size, and a borg check might not be a bad idea either as you fell asleep
and closed the mac during work on the repo.</li>
  <li>Cleaned the time machine volume of the repeated backups of the new repository
because it doesn’t make any sense to have 20 versions of it.</li>
  <li>Moved the repo to the time machine drive as temporary storage using rsync.</li>
</ul>

<h3 id="tasks-5">Tasks</h3>
<ul>
  <li>Restarted the transfer process starting on the 8th of October</li>
</ul>

<p><em><strong>Notes 13:</strong> Not a huge shock but running some of these commands across USB
2.0 can add anywhere from 10 to 30 minutes. Doing them cross device gets even
worse, with some transactions taking almost an hour.</em></p>

<p><em><strong>Notes 14:</strong> I’ve been going back and forth on what filesystem I would like
to deploy since I am redoing the collegi drive as a whole. Now the interesting
thing to note here is that by the time I get this thing fully ready to deploy,
the drive I have here may not be the drive it ends up on, but this is as good
of a testbed as any. I’m really thinking I will go with apfs. Most of the
gripes I have with it are easily resolved through borg and git annex.</em></p>

<p><em><strong>Notes 15:</strong> In a highly amusing turn of events, it is bigger in lzma 9 than
it was with gzip 9. weird.</em></p>

<p><em><strong>Notes 16:</strong> While it would likely be prudent to go back to the previous
compression method, the benefits that I have made to the directory structure
while redoing the borg repository are worth the few extra gigabytes of overhead
especially concerning with Backblaze B2 it barely costs a penny.</em></p>

<h3 id="tasks-6">Tasks</h3>

<ul>
  <li>Use JHFSX for the new drive. I would have really liked to use APFS but I am
still worried about the data loss considering there is almost a year till it
will ship. JHFSX is reasonable enough for right now, while still being safe to
unplug.</li>
  <li>I went round and round on using encryption on the new drive. did it.</li>
  <li>using rsync to bring the data to its final resting location.</li>
  <li>OK started setting things up</li>
  <li>Defined gitlab as the metadata backup again</li>
  <li>created a bare repository on skaia</li>
  <li>set up prefered content so skaia requires everything in the main repo</li>
  <li>set the main repo to require a –force to drop content via preferred content</li>
  <li>Set the backend to SHA512E</li>
  <li>began the long process of adding the data to the git-annex</li>
  <li>Set up bin directory to not be tracked by git-annex but instead by git</li>
  <li>added backblaze remote, not encrypted, with a proper prefix</li>
  <li>started to sync to backblaze</li>
  <li>noticed an issue with how the sync was going to gitlab, will correct.
    <ul>
      <li>corrected the issue</li>
    </ul>
  </li>
</ul>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2016/10/13/Collegi-Backup-Part-2/">
        Collegi Pixelmon - Backup System Part 2
      </a>
    </h1>

    <span class="post-date">13 Oct 2016</span>

    <p>What was originally intended to be a one off blog post may become my new source
of material for the coming weeks. After utilizing <a href="https://github.com/borgbackup/borg" title="BorgBackup">BorgBackup</a> and
<a href="http://git-annex.branchable.com/" title="Git-Annex">git-annex</a> to backup what has now grown to almost <a href="https://twitter.com/Zyradyl/status/786205897810780160" title="Zyradyl's Twitter">2.5 Terabytes</a> of
data, I began to wonder what other ways I could put git-annex to use for us here
at <a href="http://collegi.enjin.com" title="Collegi Pixelmon Main Website">Collegi</a>. We already use various <a href="https://gitlab.com/" title="GitLab">GitLab</a> repositories to manage
different facets of the project, and I began to wonder if there wouldn’t be some
way to use git-annex to completely unify those repositories and distribute their
information as needed.</p>

<p>This started as a brief foray into <a href="https://medium.com/@porteneuve/mastering-git-submodules-34c65e940407#.24xm3mdlt" title="Mastering Git Submodules">git submodules</a> which, while allowing me
to consolidate data <strong>locally</strong>, does nothing in helping me to properly
redistribute that data to various locations. The only way that it would be
possible to do such a thing would be to take all the various git repositories
that Collegi utilizes, which currently is sitting at <a href="https://gitlab.com/groups/collegi" title="GitLab: Collegi Group">six total</a>, including
the git-annex metadata repository (which isn’t publicly visible), and merge them
into one master repository through the use of <a href="https://medium.com/@porteneuve/mastering-git-subtrees-943d29a798ec#.r5p4ozyfm" title="Mastering Git Subtrees">git subtrees</a>. This would
allow me to still have multiple repositories for ease of project management, but
all those repositories would be pulled down, daily, to a local “master”
git-annex repository and merged into it.</p>

<p>Once this was done, the use of git annex’s <a href="https://git-annex.branchable.com/git-annex-preferred-content/" title="Git Annex: Preferred Content Manual Page">preferred content</a> system would
allow me to decide what data needed to be sent to which remote. This would let
me back up some information to one remote, and other information to another.
As an added bonus, the use of git subtrees would even allow me to push changes
back upstream, and all of it would be centralized.</p>

<p>In the future, this would allow us to push very specific data to specific team
members, who would then modify the data, which would be pulled back down on the
next git-annex sync, we would see changes needing to be pushed upstream had been
made, unlock those files, then use git subtree to push them back to their
remotes. That’s the theory at least. As far as I am aware, either no one has
done this before, no one who has done this before has lived to tell the tale, or
no one who has done this before has blogged about their experiences in doing so.</p>

<p>That’s where this blog comes in. I’m currently in the process of making a
complete copy of the current root repository, which is still using git
submodules, and from there I can begin experimenting. Whether or not this works
remains to be seen, but it coincides neatly with a rewrite of the
<a href="https://gitlab.com/collegi/collegi-backup-automation" title="GitLab: Collegi Group - Backup Automation">backup script</a> to update it to Google Shell Style Guidelines, which means
I can build the script around the new repository layout, and while doing so I
should be able to head off any unforeseen issues.</p>

<p>It’s very likely that I am going to finish writing 2.0 of the script before
doing any of this crazy shit, but this post helps me to organize my thoughts.
Besides, it just means 3.0 will be that much more exciting when it drops.</p>

<p>Stay tuned for more of my antics and adventures with making this absurd system
take shape, and turn into the omnipresent repository of every single facet of
a Minecraft community.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2016/10/01/Collegi-Backup-System/">
        The Collegi Pixelmon Server Backup System
      </a>
    </h1>

    <span class="post-date">01 Oct 2016</span>

    <p>Wow, time flies. It has been almost a year since I last updated this blog,
including fixing some of the issues that Jekyll 3.0 introduced in my formatting.
Luckily, that could be fixed by just adding a few spaces. In the past year,
quite a bit has happened, but nothing quite so exciting as becoming a co-owner
and the head developer of a new Minecraft community called <a href="http://collegi.enjin.com" title="Collegi Pixelmon Main Website">Collegi</a>. Collegi
is a <a href="http://pixelmonmod.com/" title="Pixelmon Mod Main Website">Pixelmon</a> server, which means we have Pokemon right inside Minecraft.
However, we strive to make the server Minecraft with Pokemon, instead of Pokemon
in Minecraft. It’s a small difference, but one that we happen to find very
important. We want the survival aspect of the game to be front and centre.</p>

<p>The server has become absolutely massive, with each downloaded snapshot running
about 100GB in size. (Note, that throughout this article I will be using the
SI standard GB, which is 10<sup>9</sup>, versus the Gibibyte which is
2<sup>30</sup>, how hard drive manufacturers were allowed to change the value of
a gigabyte is something I will never understand.)</p>

<p>Now, with a 500GB flash drive on my MBP, I don’t really have the room to save
all of those snapshots, especially considering we have snapshots going back six
months, across three different major versions of Minecraft. In fact, completely
expanded, the current backup amount at the time of writing is 1.11TB.</p>

<p>So, I began to search for a method of performing backups. I had some rather
strict requirements for these backups, that lead to the formulation of the
system I am going to discuss in this article.</p>

<p><strong>Requirements</strong></p>

<ul>
  <li>Incremental FTP</li>
  <li>Deduplication</li>
  <li>Compression, and the ability to modify compression levels on the fly.</li>
  <li>Checksumming to silently detect corruption.</li>
  <li>Encryption</li>
  <li>Tools need to be actively maintained and ubiquitous.</li>
  <li>Able to sync repository with a remote source.</li>
  <li>Cheap</li>
  <li>Open source wherever possible.</li>
  <li>Easy to access archived versions.</li>
  <li>Must be able to be automated.
    <ul>
      <li>If not in setup, then in how it runs later.</li>
    </ul>
  </li>
</ul>

<h2 id="step-one---getting-the-data-off-the-server">Step One - Getting the Data off the Server</h2>
<p>We use a lovely company called <a href="https://www.bisecthosting.com/" title="BisectHosting">BisectHosting</a> to run our server. They
provide an extremely barebones budget package that gives us a large amount of
our most important specification: RAM. We can live without fancy support tickets
or SSD access if they offer us cheap RAM, which they do. Beyond that, however,
they also offer unlimited disk space, as long as that disk space goes towards
the server itself, so no keeping huge numbers of backups on the server.</p>

<p>Now, they did offer a built in backup solution, but it only keeps the past seven
days available in a rolling fashion, and I really really like to keep backups.</p>

<p>The only real gripe I have about BisectHosting is that they only allow the use
of FTP for accessing data on the Budget Server tier. Worse, they don’t even use
FTP over TLS, so the authentication is in plain text. However, I just change my
password weekly and it seems to work alright.</p>

<p>The most important part of getting the data off the server is only getting the
new data, or the data that has changed. This requires using an FTP Client that
is able to sanely detect new data. Checksums aren’t available, but modification
date and file size work just as well.</p>

<p>There were a large number of clients that I tried out over time. <a href="https://filezilla-project.org/" title="Filezilla Project">Filezilla</a>
was the first of those. It seemed to work alright for a time, except that when
you have a large amount of identical files (We have 15,824 files at the time of
this writing) it hangs. Now, it does come back eventually, but it’s still not
the best of features to have a client that hangs.</p>

<p>The next one I tried was a Mac favourite known as <a href="https://cyberduck.io/" title="Cyberduck Website">Cyberduck</a>. I really liked
the interface for Cyberduck, but the first nail in its coffin was the inability
to perform a modification time comparison and a file size comparison during the
same remote to host sync. That meant it took two syncs to grab everything up to
date, and even then it didn’t always seem to take. During the time that I was
using Cyberduck, we had to restore from backup for some reason that is currently
eluding me, but when we did so we noticed that some recent changes on the map
hadn’t synced properly. Combine all of the above with the fact that from time to
time it would hang on downloads (I’m assuming from the absurd number of files)
and that wasn’t going to work.</p>

<p>The final GUI client that I tried was called <a href="https://www.panic.com/transmit/" title="Transmit Website">Transmit</a>. I really, really
enjoyed using Transmit. It is a very polished interface, but first off it
isn’t free, or open source, so that invalidated two of the requirements.
However, if it worked well enough, I was willing to overlook the issues. Problem
was, it didn’t work well. I forget what happened at the moment, but I know that
it experienced similar hanging to Filezilla.</p>

<p>Regardless, Transmit was the last GUI based client that I tried. It took me a
bit to realize, but if I used a GUI client there was a very minimal chance that
I would be able to automate the download.</p>

<p>That left command line tools, which after I found <a href="http://lftp.yar.ru/" title="LFTP Homepage">LFTP</a> I kicked myself for
not looking into first. In addition to being an open source tool, LFTP has the
ability to perform multithreaded downloads, which isn’t common in command line
clients. Furthermore, it was able to compare both modification time and file
size simultaneously, reducing the sync operations needed back to one. It is
actively maintained, available in <a href="http://brew.sh/" title="Homebrew Website">Homebrew</a> (though, at the time of writing
 it has been moved into the boneyard), written in C, and very easily scriptable.
You can call commands that would normally have to be ran from inside the FTP
client directly from the command line invokation of LFTP. It handled our data
quantity flawlessly, and easily worked through the large amount of files, though
it can take quite a while to parse our biggest directories. At the time of
writing, that directory is the map data repository for our main world, which has
12,567 items clocking in at 88.15GB. It takes between two and five minutes for
LFTP to parse the directory, which considering all the other benefits is fine
by me.</p>

<p>Our remote to local command utilizes the LFTP mirror function, and from within
the client, looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mirror -nvpe -P 5 / ~/Development/Collegi/
</code></pre></div></div>

<h2 id="step-two---convert-the-data-to-an-archive-repository">Step Two - Convert the Data to an Archive Repository</h2>
<p>When you are talking about a server that a full backup runs 100GB, and you want
to perform daily backups at minimum, it becomes absurd to think that you could
run a full backup every day. However, the notion of completely incremental
backups is far too fragile. If a single incremental backup is corrupted, every
backup after it is invalid. More than that, to access the data that was on the
server at the time the incremental was taken would require replaying every
incremental up to that point.</p>

<p>The first solution I tried for this problem was to use <a href="https://en.wikipedia.org/wiki/ZFS" title="ZFS on Wikipedia">ZFS</a>. ZFS solves
almost every problem that we have by turning on deduplication and compression,
running it on top of Apple’s <a href="https://en.wikipedia.org/wiki/FileVault" title="FileVault on Wikipedia">FileVault</a>, and utilizing <a href="https://docs.oracle.com/cd/E23824_01/html/821-1448/gbciq.html" title="Oracle's Documentation on ZFS Snapshots">snapshots</a>. The
snapshots are complete moments in time and can be mounted, and they only take
up as much space as the unique data for that snapshot. Using ZFS Snapshots, the
1.10TB of data we had at that time was reduced to 127GB on disk. Perfect. The
problem becomes, however, offsite replication.</p>

<p>Now, it is true that by having a copy of the data on the server, one on my
MacBook, and one on an external drive here at the house, the [3-2-1 Backup][12]
rule is satisfied. However, three backups of the data is not sufficient for a
server that contains over six months of work. It’s reasonable that something
cataclysmic could happen and we’d be shit out of luck. We needed another offsite
location. The only such location that offers ZFS snapshot support is
<a href="http://rsync.net/" title="Rsync.Net Homepage">Rsync.net</a> which 100% violates the “Cheap” requirement mentioned above.
That’s not a knock on their service, Rsync.net provides an incredible service,
but for our particular use case it just wasn’t appropriate.</p>

<p>So the hunt began for a deduplicating, compression based, encrypted backup
solution that stored the repository in standard files on a standard filesystem.
The final contenders were:</p>

<ul>
  <li>Using plain old <a href="https://git-scm.com/" title="Git SCM">Git</a></li>
  <li><a href="https://bup.github.io/" title="Bup: It Backs things Up!">BUP</a> (As a side note, this client has the most adorable name for a
backup utility that I have ever seen. I love it.)</li>
  <li><a href="https://attic-backup.org/" title="Attic Backup">Attic Backup</a></li>
  <li><a href="https://github.com/borgbackup/borg" title="BorgBackup">BorgBackup</a></li>
  <li><a href="http://git-annex.branchable.com/" title="Git-Annex">git-annex</a></li>
</ul>

<p>I was leaning very, very heavily toward BUP until I discovered BorgBackup. My
primary concerns with BUP was that it did not seem to be under active
development, and after over five years it still had not reached a stable 1.0.
Git would have been useful, but just like ZFS it would inevitably require a
“Smart Server” versus the presentation of just a dumb file-system.</p>

<p>BorgBackup sold me almost immediately. It allowed you to mount snapshots and
view the filesystem as it was at that time, it offers multiple levels of
compression ranging from fast and decent to slow and incredible, and it has
checksumming on top of HMAC encryption. It’s worth noting at this time that
nothing on the server is really so urgent as to require encryption, as most of
the authentication is handled by Mojang, but I still prefer to encrypt things
wherever possible.</p>

<p>It was under active development, it’s developers were active in the community
(I ended up speaking with the lead developer on twitter), and it was progressing
in a sane and stable fashion. As an added bonus, the release of 1.1 was to
provide the ability to repack already stored data, allowing us to potentially
add a heavier compression algorithm in the future and convert already stored
data over to it.</p>

<p>The only downside to Borg was that at first glance it seemed to require a Smart
server, just like git would.</p>

<p>Regardless, the system would work for now. If worst came to worst, I could
utilize something like <a href="http://rclone.org/" title="Rclone">rclone</a> to handle uploading to an offsite location.</p>

<p>When everything was said and done, we had reduced the size of our 1.11TB backup
into a sane, usable 127GB.</p>

<p>The current command that is used looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>borg create --chunker-params=10,23,16,4095 --compression zlib,9 --stats \
    --progress /Volumes/Collegi/collegi.repo::1.10.2-09292016 .
</code></pre></div></div>

<h2 id="step-three---offsite-replication">Step Three - Offsite Replication</h2>
<p>I could easily spend a very long time here discussing how I chose the cloud
provider I would inevitably use for this setup, but it really comes down to
the fact that I quite like the company, and their cloud offering has a very
complete API specification, and is dirt cheap. We went with <a href="https://www.backblaze.com/b2/cloud-storage.html" title="BackBlaze Cloud Storage">BackBlaze B2</a>.
I could, and probably will, easily write a whole separate post on how enthralled
I am with BackBlaze as a company, but more than that their $0.005/GB/Month price
is literally unbeatable. Even <a href="https://aws.amazon.com/glacier/" title="Amazon Glacier">Amazon Glacier</a> runs for $0.007/GB/Month and
they don’t offer live restoration. It’s cold storage as opposed to BackBlaze’s
live storage.</p>

<p>The problem became this: How do I get the Borg repository to fully sync to B2,
but do so in such a way that if the local repository ever became damaged I could
pull back only the data that had been lost. This is what the
<a href="http://borgbackup.readthedocs.io/en/stable/faq.html#can-i-copy-or-synchronize-my-repo-to-another-location" title="Borg Documentation: Can I sync my Repository to another Location?">documentation for Borg</a> means when it mentions you should really think
about if mirroring best meets your needs, and for us it didn’t.</p>

<p>Again though, B2 is just a storage provider, not a smart server. So how do I set
things up in this way? The answer became to use another tool that was almost
used for backup in the first place, Git-Annex. The only reason git-annex wasn’t
used for backup to begin with is that it doesn’t allow us to retain versioning
information. It just manages large files through git, which wouldn’t work.
What it would do, however, and do quite well, is to act as a layer between our
BorgBackup repository and the cloud.</p>

<p>So, I stored the entire borg repository into git annex. Once this was done, I
used a <a href="https://github.com/encryptio/git-annex-remote-b2" title="Git-Annex B2 Backend">plugin</a> for git-annex to add support for a B2 content backend. Then,
the metadata information for the git repository gets synced to <a href="https://gitlab.com/" title="GitLab">GitLab</a>, and
the content is uploaded to B2.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The end result of this is that our 100GB server, as it stands at any day, is
mirrored in four separate locations. One on the host itself, one on the MBP
hard drive, one in the Borg Repository, and one on the BackBlaze B2 Cloud. More
than that though, we have a system that is easily automated via a simple shell
script, which after completing the initial setup (sending 20,000+ files to
Backblaze B2 can take a while), I will demonstrate here.</p>

<p>Thank you so much for reading, I look forward to sharing more about the inner
workings of the Collegi Infrastructure as time permits.</p>

<h2 id="video">Video</h2>
<p>I just recently completed an asciinema of the process. See below. Also note
that you can copy and paste commands from inside the video itself. Go ahead, try
it!</p>

<script type="text/javascript" src="https://asciinema.org/a/14pvurnnazr6res7f5u1yvg7u.js" id="asciicast-14pvurnnazr6res7f5u1yvg7u" async=""></script>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2015/12/14/CPP-GUI-Library-Hell/">
        C++ GUI Libraries
      </a>
    </h1>

    <span class="post-date">14 Dec 2015</span>

    <p>So, one of the many projects that I have been working on as of late is a game
engine for a small game studio called Near The Resolution. Really awesome
people to work for, lemme tell ya. They are allowing me to release the engine
under one of my crazy “No Rights Reserved” licenses. I have learned that one
of the unfortunate parts of game design is that if you’re not using C++ then
you are essentially doomed to having to rewrite the wheel. After a lot of
research, I settled on utilizing SFML as the basis of the game engine, because
it is released under zlib.</p>

<p>One of the many, I suppose, <em>features</em> of C++ is that a number of libraries are
available for solving things without having to rewrite the wheel. This makes me
personally uncomfortable on a number of levels, but that comes with
<a href="http://warp.povusers.org/OpenLetters/ResponseToTorvalds.html">“C Hacker Syndrome”</a> which I definitely suffer from. Anyways, I decided
that I would attempt to put this.. <em>feature</em> to my advantage, and find a GUI
library that works properly with SFML, is clean and simple, and is small.</p>

<p>Turns out, this isn’t actually possible.</p>

<h3 id="sfgui"><a href="http://sfgui.sfml-dev.de">SFGUI</a></h3>
<p>SFGUI is the first thing I spent time working on. The problem with SFGUI is it
suffers from “There’s a fleck on the speck on the tail on the frog on the bump
on the branch on the log in the hole in the bottom of the sea” Syndrome. If you
want to make a button, for example.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Create the label.
m_label = sfg::Label::Create( "Hello world!" );

// Create a simple button and connect the click signal.
auto button = sfg::Button::Create( "Greet SFGUI!" );
button-&gt;GetSignal( sfg::Widget::OnLeftClick ).Connect( std::bind( &amp;HelloWorld::OnButtonClick, this ) );

// Create a vertical box layouter with 5 pixels spacing and add the label
// and button to it.
auto box = sfg::Box::Create( sfg::Box::Orientation::VERTICAL, 5.0f );
box-&gt;Pack( m_label );
box-&gt;Pack( button, false );

// Create a window and add the box layouter to it. Also set the window's title.
auto window = sfg::Window::Create();
window-&gt;SetTitle( "Hello world!" );
window-&gt;Add( box );

// Create a desktop and add the window to it.
sfg::Desktop desktop;
desktop.Add( window );

// We're not using SFML to render anything in this program, so reset OpenGL
// states. Otherwise we wouldn't see anything.
render_window.resetGLStates();
</code></pre></div></div>

<p>So you want a button. There’s a Label on the button in the box on the window
on the desktop in the SFML window on the openGL at the bottom of the code tree.
Needless to say, this lasted about five minutes in our source tree before I
broke down sobbing. Not to mention, the entire system renders in its own area
of the main SFML window. So when I tried to make an SFGUI button in the SFML
window on top of our background, yeah it didn’t go to well, and I don’t need
<em>another</em> engine on top of the engine on top of this game engine. That’s just..
absurd. Not to mention the documentation sucks. I hate crappy documentation.</p>

<h3 id="tgui"><a href="https://tgui.eu">TGUI</a></h3>
<p>TGUI bills itself as the “Simple GUI” for SFML. So naturally, I was really
quite excited to try it out. Until I figured out it required configuration
files, and every button had to be an image or it wouldn’t render. So in theory,
you couldn’t have a completely transparent button, which is one of the things
that this game will require. Secondly, if you require a configuration syntax,
you are <em>not</em> a simple library. Ever. All I want is to be able to call methods
that create a button.</p>

<h3 id="cegui"><a href="http://cegui.org.uk">CEGUI</a></h3>
<p>Desperate times call for desperate measures. Or so I believed. Problem is,
CEGUI actually contains its own OpenGL renderer, which is redundant on top of
SFML. There is never a reason to have two OpenGL renderers, they will conflict.
Furthermore, CEGUI requires XML. Which is -not- a simple library. They claim
that this is to reduce the frustration of having to change your code to adjust
the GUI interface, but I would rather adjust my code than end up dependent on
XML files.</p>

<h3 id="conclusion">Conclusion</h3>
<p>At the end of the day, All I really need to do for this particular engine is
lay out a few menus and then a general presentation on top of those for the
in game content. So I used <a href="http://www.bromeon.ch/libraries/thor/">Thor</a> to render shapes, and then I’m writing my
own logic on top of that. This gives me total control over the implementation,
and the logic of the GUI system. Maybe I’m strange in that I don’t mind having
to change values in my code and recompile, but I honestly prefer to do that.
Control over individual placement makes sense, and most of the time I define
my placement on top of positioning logic, so in those cases it makes more sense
to have it in the code than in an external file.</p>


  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page/3/">Older</a>
  
  
    
      <a class="pagination-item newer" href="">Newer</a>
    
  
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
