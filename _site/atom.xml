<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Bit of A Byte</title>
 <link href="http://zyradyl.github.io/atom.xml" rel="self"/>
 <link href="http://zyradyl.github.io/"/>
 <updated>2018-12-10T01:04:21-06:00</updated>
 <id>http://zyradyl.github.io/</id>
 <author>
   <name>Natalie M. Spencer</name>
   <email>nmspencer89@gmail.com</email>
 </author>

 
 <entry>
   <title>More Blog Updates</title>
   <link href="http://zyradyl.github.io/2018/12/10/More-Blog-Updates/"/>
   <updated>2018-12-10T01:00:00-06:00</updated>
   <id>http://zyradyl.github.io/2018/12/10/More-Blog-Updates</id>
   <content type="html">&lt;p&gt;There are yet more updates to the blog. The first of which is that we now have
an actual domain name, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;zyradyl.moe&lt;/code&gt;. In keeping with my tradition of
complete transparency, the domain was acquired through Gandi.net, after I found
out that IWantMyName was unable to accept Discover cards. While I am still
supportive of IWMN as a company, if they don’t accept my card it leaves me
unable to use them.&lt;/p&gt;

&lt;p&gt;Next, the DNS for this site is now handled through Cloudflare, which means that
this site is now fully available via HTTPS with a valid SSL certificate. So,
small victories.&lt;/p&gt;

&lt;p&gt;While running through the process of updating the blog, I noticed several things
were broken and went ahead and fixed those:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The “Site Version” link in the sidebar now properly links to the GitHub
source repository.&lt;/li&gt;
  &lt;li&gt;A long standing issue with pagination has been corrected by updating to
the jekyll-paginate-v2 gem, and rewriting the appropriate liquid blocks.&lt;/li&gt;
  &lt;li&gt;Related posts are now actually related! This is accomplished by iterating
through tags at compile time and creating a list of related posts. While
this may not always be accurate, it is far more accurate than the time
based system jekyll uses by default.&lt;/li&gt;
  &lt;li&gt;A small issue has been corrected with the header file used across pages.
There was a typo that was generating invalid HTML. It didn’t cause any
visible issues, but it was a problem all the same.&lt;/li&gt;
  &lt;li&gt;The archive page now uses a new Liquid code block. This is to resolve the
long standing &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;/ul&amp;gt;&lt;/code&gt; problem, where the code would generate trailing
closing tags.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HTML proofer is still throwing a few errors related to my consistent use of
the Introduction and Conclusion headers, but these are not actual errors.&lt;/p&gt;

&lt;p&gt;I’m also in the process of going back through previous posts and cleaning up
the YAML front matter. While this front-matter previously had very little
impact on the site, it now can matter quite a lot with the way the related
posts system works.&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ThothBackup - Part 3</title>
   <link href="http://zyradyl.github.io/2018/12/09/ThothBackup-Part-3/"/>
   <updated>2018-12-09T17:00:00-06:00</updated>
   <id>http://zyradyl.github.io/2018/12/09/ThothBackup-Part-3</id>
   <content type="html">&lt;p&gt;So, another week has gone, and it is time to update this blog with what I have
learned. Unfortunately, experiments were not able to be run this week in the
realm of data transfer. I decided to revisit the base system to focus on
encrypting backup data while it is at rest on the system. This was one of the
remaining security vulnerabilities with this process. While end-users still have
to trust &lt;em&gt;me&lt;/em&gt;, they can at least be assured the data is encrypted at rest.&lt;/p&gt;

&lt;p&gt;Essentially, if the system was ever stolen, or our apartment door was broken
down, we would just have to cut power and the data would be good. With that
previous statement, please keep in mind that this week’s post only refers to the
root drive. I didn’t make much progress because of things happening at work, but
this is a nice, strong, foundation to build upon.&lt;/p&gt;

&lt;p&gt;Many of the steps in this post were cobbled together from various sources across
the internet. At the bottom of this post you can find a works cited that will
show the posts that I used to gather the appropriate information.&lt;/p&gt;

&lt;h2 id=&quot;end-goal&quot;&gt;End Goal&lt;/h2&gt;
&lt;p&gt;The end goal is to ensure that the operating system’s root drive is encrypted at
rest. Full Disk Encryption is &lt;em&gt;not&lt;/em&gt; an active security measure, it is a passive
one. It is primarily there to ensure that should the system ever be stolen, it
would not be readable. The root partition will not host any user data, so the
encryption should be transparent and seamless.&lt;/p&gt;

&lt;p&gt;In short, we will utilize a USB key to provide a Keyfile which will then be
combined with LUKS encryption to unlock the LVM array to allow the initramfs to
hand over control to the operating system.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;p&gt;Because we are using a solid state drive, and we will be filling the drive with
data, it was important for me to over-provision the drive. The SSD we’re using
comes with 240GB of space. We can assume that there is some form of manufacturer
over-provisioning in play to get that number, if I had to guess I would assume
there is actually 256GB of NAND memory on the drive, but only 240GB are made
available to the user. This is a fairly reasonable level of over-provisioning.&lt;/p&gt;

&lt;p&gt;However, with us planning to fill the drive with pseudorandom data in order to
obfuscate the amount of data actually in use, this 16GB could potentially be
used quite quickly. SSDs cannot actually rewrite sectors on the fly, they have
to run a READ/ERASE/WRITE cycle. This is typically done by writing the new block
to an over-provisioned area and then pointing the drive’s firmware at that
block. In this way we avoid the ERASE penalty, which can be on the order of 0.5
seconds per block.&lt;/p&gt;

&lt;p&gt;Essentially then, every single write to the drive will require a
READ/ERASE/WRITE cycle, so padding the over-provisioning is a very good idea. It
will help with wear leveling and prevent severe write amplification, while also
making the drive “feel” faster.&lt;/p&gt;

&lt;h2 id=&quot;prior-work&quot;&gt;Prior Work&lt;/h2&gt;
&lt;p&gt;Before we get into the new installation, we need to prepare the drive for its
new role. Unless the flash cells are at their default state, the firmware will
regard them as holding data and will not utilize them for wear leveling, thus
rendering the over-provisioning useless.&lt;/p&gt;

&lt;p&gt;To begin, boot the system via a Debian Live-CD and open up a root prompt using
&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you, like me, prefer to work remotely, you will then need to run a sequence
of commands to prep the system for SSH access. We need to add a password to the
liveCD user, then install openSSH, and finally start the service. Once all of
this is complete, you can log in from a more comfortable system.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# apt-get update
# apt-get install openssh-server
# passwd user
# systemctl start sshd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will need to install one last software package, &lt;code class=&quot;highlighter-rouge&quot;&gt;hdparm&lt;/code&gt;. Run
&lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get install hdparm&lt;/code&gt; to grab it. Once you have done so, run
&lt;code class=&quot;highlighter-rouge&quot;&gt;hdparm -I /dev/sda&lt;/code&gt;. Under “Security” you are looking for the words “&lt;strong&gt;not&lt;/strong&gt;
frozen”. If it says frozen, and you are working remotely, you will need to
access the physical console to suspend/resume the machine. This should
unfreeze access to the ATA security system.&lt;/p&gt;

&lt;p&gt;The first thing we need to do is to run an ATA Enhanced Erase. After this is
done, I still like to run &lt;code class=&quot;highlighter-rouge&quot;&gt;blkdiscard&lt;/code&gt; just to make sure every sector has been
marked as empty. Finally, we will use &lt;code class=&quot;highlighter-rouge&quot;&gt;hdparm&lt;/code&gt; to mark a host-protected-area,
which the drive firmware will be able to use as an over-provisioning space.
To calculate the HPA size, figure out what size you want to be available to
&lt;em&gt;you&lt;/em&gt;. Convert that into bytes, and divide by 512, which is the sector size.
This will give you the number to pass to &lt;code class=&quot;highlighter-rouge&quot;&gt;hdparm&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# hdparm --user-master u --security-set-pass Eins /dev/sda
# hdparm --user-master u --security-erase-enhanced Eins /dev/sda
# blkdiscard /dev/sda
# hdparm -Np390625000 --yes-i-know-what-i-am-doing /dev/sda
# reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once this is done &lt;strong&gt;reboot immediately&lt;/strong&gt;. There is a lot that can go wrong if
you fail to reboot. At this point, I swapped out my disk for the Debian
installer. If you are doing this on your own 2006-2008 MacMini, you may want
to use the AMD64-mac ISO that the Debian project provides.&lt;/p&gt;

&lt;p&gt;From here, we just have to confirm that the drive shows up how we want in the
installer (200GB in size, in my case), and we can proceed with the installation.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;
&lt;p&gt;Most of the Debian installation process is self explanatory. The only point
where I will interject is partitioning. Because of the way the MacMini2,1
boots, it is important that we use an MBR based grub installation. You &lt;em&gt;can&lt;/em&gt; do
a 32bit EFI installation, but it is very fragile, and I’m not a fan of fragile
things. That being said, I still wanted the ability to use GPT partitions. I
like being able to label everything from the partition up to the individual
filesystems.&lt;/p&gt;

&lt;p&gt;Accomplishing this is actually fairly easy anymore. You just need to create a
1MB &lt;code class=&quot;highlighter-rouge&quot;&gt;grub_bios&lt;/code&gt; partition as part of your scheme and you’re good to go. To get
the level of control we need, we will select manual partitioning when prompted
to set up our partitions in the installer.&lt;/p&gt;

&lt;p&gt;Create a new partition table (This will default to GPT), and then lay out your
initial partition layout. It will look something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;PART #&amp;gt;  &amp;lt;SIZE&amp;gt;  &amp;lt;NAME&amp;gt;          &amp;lt;FILESYSTEM&amp;gt;  &amp;lt;FLAGS&amp;gt;
#1        1MB     BIOS_PARTITION  none          grub_bios
#2        1GB     BOOT_PARTITION  ext4          bootable
#3        199GB   ROOT_PARTITION  crypto        crypto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you select “Physical Volume For Encryption” it will prompt you to configure
some features. You can customize the partition there, but I actually wanted more
options than the GUI provided, so I accepted the defaults and planned to
re-encrypt later. Please make sure to allow the installer to write encrypted
data to the partition. Since we have already set up a customized HPA, a
potential attacker already knows the maximum amount of cipher text that can
be present, and if the HPA is disabled they would likely be able to gain
access to more. Therefore, it is important that we take every possible
precaution.&lt;/p&gt;

&lt;p&gt;Once this is done, you should scroll to the top where it will say “Configure
Encryption” or something similar. Select this option, then select the physical
volume we just set up, and it should drop you back to the partitioning menu.
This time, however, you will be able to see the newly unlocked crypto partition
as something that we can further customize.&lt;/p&gt;

&lt;p&gt;Select that volume and partition it like so:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;PART #&amp;gt;  &amp;lt;SIZE&amp;gt;  &amp;lt;NAME&amp;gt;          &amp;lt;FILESYSTEM&amp;gt;  &amp;lt;FLAGS&amp;gt;
#1        199GB                   none          lvm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The LVM option will show up in the menu as “Physical Volume for LVM.” From here,
we go back up to the top of our menu and select “Configure Logical Volume
Manager.” You will then be taken to a new screen where it should show that you
have one &lt;code class=&quot;highlighter-rouge&quot;&gt;PV&lt;/code&gt; available for use. Create a new volume group that fills the entire
&lt;code class=&quot;highlighter-rouge&quot;&gt;PV&lt;/code&gt; and name it as you would like. For this project, I named it &lt;code class=&quot;highlighter-rouge&quot;&gt;djehuti-root&lt;/code&gt;
and completed setup.&lt;/p&gt;

&lt;p&gt;Next we need to create a Logical Volume for each partition that you would like
to have. For me, this looked like the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Logical Volume&amp;gt;  &amp;lt;Size&amp;gt;  &amp;lt;Name&amp;gt;
#1                30GB    root-root
#2                25GB    root-home
#3                10GB    root-opt
#4                05GB    root-swap
#5                05GB    root-tmp
#6                10GB    root-usr-local
#7                10GB    root-var
#8                05GB    root-var-audit
#9                05GB    root-var-log
#10               05GB    root-var-tmp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Your layout may be similar. Once this is done, you can exit out and you will
see that all of your logical volumes are now available for formatting. Since I
wanted to stick with something stable, and most importantly resizable (more on
why later), I picked &lt;code class=&quot;highlighter-rouge&quot;&gt;ext4&lt;/code&gt; for all of my partitioning. We will tweak mount
options later. For now, the end product looked like the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;PARTITION&amp;gt;                       &amp;lt;FS&amp;gt;    &amp;lt;MOUNT POINT&amp;gt; &amp;lt;MOUNT OPTIONS&amp;gt;
/dev/sda2                         ext4    /boot         defaults
/dev/djehuti-root/root-root       ext4    /             defaults
/dev/djehuti-root/root-home       ext4    /home         defaults
/dev/djehuti-root/root-opt        ext4    /opt          defaults
/dev/djehuti-root/root-swap       swapfs  none          defaults
/dev/djehuti-root/root-tmp        ext4    /tmp          defaults
/dev/djehuti-root/root-usr-local  ext4    /usr/local    defaults
/dev/djehuti-root/root-var        ext4    /var          defaults
/dev/djehuti-root/root-var-audit  ext4    /var/audit    defaults
/dev/djehuti-root/root-var-log    ext4    /var/log      defaults
/dev/djehuti-root/root-var-tmp    ext4    /var/tmp      defaults
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once everything is setup appropriately, follow through the installation until
you get to the &lt;code class=&quot;highlighter-rouge&quot;&gt;task-sel&lt;/code&gt; portion. You really only want to install an ssh server
and the standard system utilities pack. Once the installation completes, reboot
into your server and make sure everything boots appropriately. We’re going to be
doing some offline tweaking after this point, so ensuring that everything is
functioning as is will save you a lot of headache.&lt;/p&gt;

&lt;p&gt;Once you are satisfied the initial installation is functioning and booting
correctly, it is time to move on to re-encrypting the partition with our own
heavily customized parameters.&lt;/p&gt;

&lt;h2 id=&quot;re-encryption&quot;&gt;Re-Encryption&lt;/h2&gt;
&lt;p&gt;This process isn’t so much difficult as it is simply time consuming. Go ahead
and reboot your system to the boot media selection screen. You will want to
swap out your Debian Installation CD for the Debian LiveCD that we used earlier.
Once the disks have been swapped, boot into the live environment and then
bring up a shell. We will first need to install the tools that we will use, and
then run the actual command. The command is actually fairly self explanatory,
so I won’t explain that, but I will explain the reasoning behind the parameters
below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# apt-get update
# apt-get install cryptsetup
# cryptsetup-reencrypt /dev/sda3 --verbose --use-random --cipher serpent-xts-plain64 --key-size 512 --hash whirlpool --iter-time &amp;lt;higher number&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, onto the parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;cipher&lt;/em&gt;    - I picked Serpent because it is &lt;a href=&quot;https://en.wikipedia.org/wiki/Serpent_(cipher)&quot;&gt;widely acknowledged&lt;/a&gt; to be
              a more “secure” cipher. Appropriate text from the above link
              is as follows: “The official NIST report on AES competition
              classified Serpent as having &lt;strong&gt;a high security margin&lt;/strong&gt; along
              with MARS and Twofish, &lt;strong&gt;in contrast to the adequate security
              margin of RC6 and Rijndael (currently AES)&lt;/strong&gt;.” The speed
              trade-off was negligible for me, as the true bottleneck in the
              system will be network speed, not disk speed.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;key-size&lt;/em&gt;  - The XTS algorithm requires double the number of bits to
              achieve the same &lt;a href=&quot;https://en.wikipedia.org/wiki/Disk_encryption_theory#XTS&quot;&gt;level of security&lt;/a&gt;. Therefore, 512 bits
              are required to achieve an AES-256 level of security.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;hash&lt;/em&gt;      - In general, I prefer hashes that have actually had extensive
              cryptanalysis performed to very high round counts. The best
              example of an attack on whirlpool, with a worst case situation
              where the attacker controls almost all aspects of the hash,
              the time complexity is still 2^128th on 9.5 of 10 rounds. This
              establishes a &lt;em&gt;known&lt;/em&gt; time to break of over 100 years.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;iter-time&lt;/em&gt; - The higher your iteration time, the longer it takes to unlock,
              but it also makes it harder to break the hash function. So if
              we combine what we know above with a large iteration time, we
              gain fairly strong security at the expense of a long unlock
              time when using a passphrase.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once these specifications have been entered, you simply need to press enter and
sit back and relax as the system handles the rest. Once this process is
complete, you should once again reset and boot into the system to verify that
everything is still working as intended. If it is, you are ready for the next
step, which is automating the unlock process.&lt;/p&gt;

&lt;h2 id=&quot;auto-decryption&quot;&gt;Auto-Decryption&lt;/h2&gt;
&lt;p&gt;There are a few ways to handle USB key based auto-decryption. The end goal is
to actually use a hardware security module to do this, and I don’t anticipate
the FBI busting down my door any time soon for hosting the data of my friends
and family, so I opted for one that is easily extendable.&lt;/p&gt;

&lt;p&gt;Essentially, the key will live on an &lt;code class=&quot;highlighter-rouge&quot;&gt;ext4&lt;/code&gt; filesystem. It will be a simple
hidden file, so nothing extremely complex to find. This shouldn’t be considered
secure at &lt;em&gt;this point&lt;/em&gt;, but it is paving the way to a slightly more secure
future.&lt;/p&gt;

&lt;p&gt;The first thing that I did, though it isn’t strictly necessary, is write random
data to the entire USB stick. In my case, the USB drive could be found at
&lt;code class=&quot;highlighter-rouge&quot;&gt;/dev/sdb&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# dd if=/dev/urandom of=/dev/sdb status=progress bs=1M
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once this is done, we’ve effectively destroyed the partition table. We will
recreate a GPT table, and then create a partition that fills the usable space
of the drive.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# apt update
# apt install parted
# parted /dev/sdb
(parted) mklabel gpt
(parted) mkpart KEYS ext4 0% 100%
(parted) quit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we just create the filesystem, a mount point for the filesystem, and make
our new LUKS keyfile. Once the file has been created, we just add it to the
existing LUKS header.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkfs.ext4 -L KEYS /dev/sdb1
# mkdir /mnt/KEYS
# mount LABEL=KEYS /mnt/KEYS
# dd if=/dev/random of=/mnt/KEYS/.root_key bs=1 count=4096 status=progress
# cryptsetup luksAddKey /dev/sda3 /mnt/KEYS/.root_key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this point, the setup diverges a bit depending on what guide you follow.
We will stick close to the guide posted to the Debian mailing list for now, as
that guide got me a successful boot on the first try. The others are slightly
more elegant looking, but at the expense of added complexity. As such, they may
end up being the &lt;em&gt;final&lt;/em&gt; configuration, but for this prototyping phase they are
a bit excessive.&lt;/p&gt;

&lt;p&gt;We have to modify the &lt;code class=&quot;highlighter-rouge&quot;&gt;crypttab&lt;/code&gt; file to enable the keyfile to be loaded off of
our freshly set up key drive.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sda3_crypt  UUID=&quot;...&quot;  /dev/disk/by-label/KEYS:/.root_key:5  luks,initramfs,keyscript=/lib/cryptsetup/scripts/passdev,tries=2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, we need to repackage our startup image, update grub, and reboot
to test the whole package.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# update-initramfs -tuck all
# update-grub
# reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point the system should boot automatically, but you will notice a weird
&lt;code class=&quot;highlighter-rouge&quot;&gt;systemd&lt;/code&gt; based timeout that happens. This is mentioned in the guide posted to
the Debian Stretch mailing list, and is fairly easy to solve. We just need to
create an empty service file to prevent &lt;code class=&quot;highlighter-rouge&quot;&gt;systemd&lt;/code&gt; from doing it’s own thing.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# touch &quot;/etc/systemd/system/systemd-cryptsetup@sda3_crypt.service&quot;
# reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, everything should boot correctly and quickly. You may notice a
few thrown errors, but it shouldn’t be anything severe, more services loading
out of order.&lt;/p&gt;

&lt;p&gt;At this point, it used to be possible to allow for the creation of a fallback
in the event that the key drive wasn’t present, but that seems to have been
removed. I plan to look into it further when I have more time.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This concludes the first part of the Operating System setup process. The next
step was originally planned to be thin-provisioning the partitions inside the
&lt;code class=&quot;highlighter-rouge&quot;&gt;djehuti-root&lt;/code&gt; volume group, but there seems to be some problems in getting
the system to boot from a thin-provisioned root. I’m looking into a weird
combined system, where the root is static but all the accessory partitions are
thinly provisioned, but it will take time to tinker with this and report back.&lt;/p&gt;

&lt;p&gt;Thin Provisioning isn’t strictly required, but it is a rather neat feature and
I like the idea of being able to create more partitions than would technically
fit. I’m not sure when this would be useful, but we will see.&lt;/p&gt;

&lt;p&gt;Once all of this is finalized, we will move on to hardening the base system,
and last but not least creating the Stage 1 Project page. Then it is back to
experiments with data synchronization. This is a fairly large step back in
progress, but I am hopeful it will result in a better end product, where
security can be dynamically updated as needed.&lt;/p&gt;

&lt;h2 id=&quot;works-cited&quot;&gt;Works Cited&lt;/h2&gt;
&lt;p&gt;The following sources were invaluable in cobbling this process together. I
sincerely thank the authors both for figuring the process out and documenting
the process online.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.debian.org/debian-user/2017/12/msg00523.html&quot;&gt;Debian Stretch - USB Keyfile with LUKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wiki.archlinux.org/index.php/Securely_wipe_disk&quot;&gt;Arch Wiki - Secure Disk Wiping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wiki.archlinux.org/index.php/Dm-crypt/Encrypting_an_entire_system&quot;&gt;Arch Wiki - Encrypting an Entire Disk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wiki.gentoo.org/wiki/Sakaki%27s_EFI_Install_Guide/Preparing_the_LUKS-LVM_Filesystem_and_Boot_USB_Key&quot;&gt;Gentoo Wiki - Sakaki’s EFI Install Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aaronbonner.io/post/21103731114/chroot-into-a-broken-linux-install&quot;&gt;Chroot Into Broken Linux Install&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mattgadient.com/2016/07/11/linux-dvd-images-and-how-to-for-32-bit-efi-macs-late-2006-models/&quot;&gt;Linux Images for 32Bit EFI Macs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mattgadient.com/2018/02/12/reducing-the-30-second-delay-when-starting-64-bit-ubuntu-in-bios-mode-on-the-old-32-bit-efi-macs/&quot;&gt;Reducing 30 Second Delay when Booting Linux&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://support.siliconmechanics.com/portal/kb/articles/over-provisioning-ssds&quot;&gt;Over-Provisioning SSDs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>ThothBackup - Part 2</title>
   <link href="http://zyradyl.github.io/2018/12/02/ThothBackup-Part-2/"/>
   <updated>2018-12-02T20:30:00-06:00</updated>
   <id>http://zyradyl.github.io/2018/12/02/ThothBackup-Part-2</id>
   <content type="html">&lt;p&gt;Hello! It’s that time of the week again, where I update everyone on my latest
work. This episode is far less technical and focuses more on the concept of a
“One and Done” backup solution, aka the holy grail of data maintenance.&lt;/p&gt;

&lt;p&gt;It fucking sucks.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;This entry is slightly unidirectional. The concept of a simple, easy to
implement, catch everything you might ever need solution is quite literally the
holy grail, yet it has never honestly been implemented. Sure, user data is
generally scooped out, but in the day and age of game mods, and with some
development projects taking place outside of the User directory, it seemed
prudent to at least &lt;em&gt;attempt&lt;/em&gt; the full backup. Well, I’ve been attempting it
for seven days. Here’s what I’ve found.&lt;/p&gt;

&lt;h3 id=&quot;focus&quot;&gt;Focus&lt;/h3&gt;
&lt;p&gt;We will not be focusing on the space impact of a complete backup. This is
actually fairly negligible. With out-of-band deduplication, only one set of
operating system files would ever be stored, so server side storage would reach
a weird type of equilibrium fairly quickly. Instead, I’ll talk about three
things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Metadata Overhead&lt;/li&gt;
  &lt;li&gt;Metadata Processing&lt;/li&gt;
  &lt;li&gt;Initial Synchronization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There may be another post tonight talking about additional things, but this
deserves it’s own little deal.&lt;/p&gt;

&lt;h3 id=&quot;metadata-overhead&quot;&gt;Metadata Overhead&lt;/h3&gt;
&lt;p&gt;A fully updated Windows 10 partition of your average gamer, aka my fiancé, is
composed of &lt;strong&gt;479,641 files&lt;/strong&gt; and &lt;strong&gt;70,005 directories&lt;/strong&gt; which comprise a total
data size of &lt;strong&gt;~216 GiB&lt;/strong&gt;. This is actually just the C drive and typical
programs. If you factor in the actual game drive in use by our test case, that
drive contains &lt;strong&gt;354,315 files&lt;/strong&gt; and &lt;strong&gt;29,111 directories&lt;/strong&gt; which comprise a
total of &lt;strong&gt;~385 GiB&lt;/strong&gt; of space.&lt;/p&gt;

&lt;p&gt;In summation, an initial synchronization of what is typically considered a “full
system backup” comprises &lt;strong&gt;833,956 files&lt;/strong&gt; and &lt;strong&gt;99116 directories&lt;/strong&gt; comprising
&lt;strong&gt;~601GiB&lt;/strong&gt; which results in an average filesize of &lt;strong&gt;~755KiB&lt;/strong&gt; and an average
directory size of &lt;strong&gt;~9 files&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;SyncThing creates a block store that is comprised of, by default, &lt;strong&gt;128KiB&lt;/strong&gt;
blocks. This means that for our system, assuming the data is contiguous, we need
&lt;strong&gt;4923392 Metadata Entries&lt;/strong&gt;. Assuming the files are NOT contiguous, this is
probably closer to about &lt;strong&gt;5 Million&lt;/strong&gt; metadata entries. As of right now, the
server side metadata storage for the testing pool is at &lt;strong&gt;1.7 GiB&lt;/strong&gt; and initial
syncronization is &lt;strong&gt;not yet complete&lt;/strong&gt;. Extrapolating a bit, we can assume that
&lt;strong&gt;2.0 GiB&lt;/strong&gt; would not be an unreasonable size for a final server side data
store.&lt;/p&gt;

&lt;p&gt;The client side store, at the time of writing, is approximately &lt;strong&gt;1 GiB&lt;/strong&gt; and
may grow slightly larger. However, I will use &lt;strong&gt;1 GiB&lt;/strong&gt;. This means that there
is a plausible total of &lt;strong&gt;3GiB of metadata overhead&lt;/strong&gt; representing an overhead
percentage of &lt;strong&gt;~0.5%&lt;/strong&gt; across the pool. Scaling up, this means 10 clients
with 1TB of data each would require &lt;strong&gt;51.2GB of Metadata&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Should anything happen to the metadata store, it would need to be rebuilt by
data reprocessing. This introduces a potentially massive liability, as scanning
frequency would need to be reduced to not impact the rebuild operation.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;metadata-processing&quot;&gt;Metadata Processing&lt;/h3&gt;
&lt;p&gt;The server is capable of a hash rate of &lt;strong&gt;107MB/s&lt;/strong&gt;. I am picking the server’s
hash rate because it is both the slowest hash rate of the pool and would have
the most metadata that would need to be rebuilt.&lt;/p&gt;

&lt;p&gt;For a complete rebuild of the data of our current cluster, it would take the
server &lt;strong&gt;~96 Minutes&lt;/strong&gt; during which no data synchronization could occur. This
equates to a minimum of &lt;strong&gt;1 Missed Hourly Update&lt;/strong&gt; and could potentially result
in up to 2 missed hourly updates if the timing was unfortunate enough.&lt;/p&gt;

&lt;p&gt;For a complete rebuild of the data of our theoretical cluster, we will allow for
a hash rate of &lt;strong&gt;300MB/s&lt;/strong&gt;. The total data needed to be rebuilt would be 10TB.
This would result in a database rebuilt time of &lt;strong&gt;~10 Hours&lt;/strong&gt; which could result
in up to 11 missed synchronization attempts.&lt;/p&gt;

&lt;h3 id=&quot;initial-synchronization&quot;&gt;Initial Synchronization&lt;/h3&gt;
&lt;p&gt;The initial syncronization is composed of three primary parts. First, the
client and host must agree on what folders to syncronize. Second, the client
must build a database of the content hosted locally. Next, utilizing a rolling
hash algorithm, data is entered into the metadata cache and transmitted to the
server.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Per the developer of SyncThing, millions of small files are the worst case
scenario for the backup system.&lt;/em&gt; As of my independent, albeit anecdotal testing,
After 7 days the synchronization process is still in effect. This represents a
very &lt;strong&gt;poor user experience&lt;/strong&gt; and would not be ideal for a widespread rollout.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The primary goal of a backup utility is to synchronize files and achieve cross
system consistency as quickly as possible. While it is true that eventually
consistent systems are utilized in large scale operations, this type of
consistency is allowable only, in my opinion, at data sizes over 10TB. The
current testing set is approximately 1TB at most, and thus this is unacceptable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Either the backup paradigm must change, or the utility used to implement it
must change.&lt;/strong&gt; While I do not expect to find any faster utilities for performing
the backup process, I do plan to continue to experiment. At this time, however,
it seems that the most likely way to make the process as friendly as possible
would be the implementation of a default backup subset, with additional data
added upon user request, and after the high priority synchronization had been
completed.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Blog Updates</title>
   <link href="http://zyradyl.github.io/2018/11/30/Blog-Updates/"/>
   <updated>2018-11-30T01:30:00-06:00</updated>
   <id>http://zyradyl.github.io/2018/11/30/Blog-Updates</id>
   <content type="html">&lt;p&gt;Yes, it’s that time of year again. I have updated the blog! You can find more
information below.&lt;/p&gt;

&lt;h4 id=&quot;analytics&quot;&gt;Analytics&lt;/h4&gt;
&lt;p&gt;Google Analytics are back. I understand that some people may not like being
tracked, but at that point you should have an add or tracking blocker installed.
I recommend looking at the Brave Browser, which is what I personally use, or
installing uBlock Origin. The reason I have added this back to the blog is that
I have noticed links to this blog appearing in various places over the web, and
I would like to be able to detect how much traffic I am getting from these
links.&lt;/p&gt;

&lt;p&gt;I wholeheartedly understand if you disapprove of the use of Google Analytics. If
anyone is able to suggest a better service, that collects less user data, please
open an issue in the GitHub repository for this site. I will gladly change
providers as long as the new one is also free.&lt;/p&gt;

&lt;h4 id=&quot;theme-updates&quot;&gt;Theme Updates&lt;/h4&gt;
&lt;p&gt;I forked the Lanyon repository and applied all the currently pending pull
requests. This should keep everything up to date with the latest version of
Jekyll. To make it easier for anyone else looking for that information, I
created a new PR in the lanyon repository to my mergers. You can also find them
under my GitHub site.&lt;/p&gt;

&lt;h4 id=&quot;fonts&quot;&gt;Fonts&lt;/h4&gt;
&lt;p&gt;Somehow the fonts on the site got nuked during the upgrade. They are back in
place now.&lt;/p&gt;

&lt;h4 id=&quot;page-speed&quot;&gt;Page Speed&lt;/h4&gt;
&lt;p&gt;The new updates have not yet been optimized. Previously I used Google’s page
load speed thingy to optimize the site. Maintaining that by hand is one of the
reasons the upgrade was so painful to implement. I’m looking for a way to
automate the process the same way that I have currently automated the link tests
that I run prior to a push. This will likely involve writing a new process in
the Rakefile, so it will take some time. In the meantime, the only thing that
is really being pulled is a few font files and the analytics script, so the
load impact &lt;em&gt;shouldn’t&lt;/em&gt; be too bad.&lt;/p&gt;

&lt;h4 id=&quot;organization&quot;&gt;Organization&lt;/h4&gt;
&lt;p&gt;There is a weird issue between rendering the site on my local machine and the
way GitHub pages renders it on their side. To solve this I created a new
template and appended it to all the pages that should not appear in the sidebar.
Hopefully this strikes a good balance between being able to use standardized
templates, as well as ease of use. The new template simply imports the existing
page template under a new name.&lt;/p&gt;

&lt;h4 id=&quot;images&quot;&gt;Images&lt;/h4&gt;
&lt;p&gt;While I am primarily focused on text on this blog, I have recently included a
few images. These are also hosted on GitHub, so I cleaned up the way the image
directory is laid out. This has impacted all of one image, but should make any
expansion in the future much easier.&lt;/p&gt;

&lt;h4 id=&quot;liquid-changes&quot;&gt;Liquid Changes&lt;/h4&gt;
&lt;p&gt;There was some issue with the way the Liquid on the Tags page was written. This
has been corrected accordingly.&lt;/p&gt;

&lt;h4 id=&quot;about-page&quot;&gt;About Page&lt;/h4&gt;
&lt;p&gt;The about page has been correctly updated! My view on some of the listed issues
has evolved in recent years. You will now find those things struckthrough with
comments added underneath.&lt;/p&gt;

&lt;h4 id=&quot;future-improvements&quot;&gt;Future Improvements&lt;/h4&gt;
&lt;p&gt;There is still minifying, javascript inlining, and font work to be done to make
the page run faster. Additionally, the tags page is simply a disaster. Between
all of that and the project pages, there is still a lot left to be done.
Hopefully, all will be accomplished in time.&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Hopefully all of these changes make the site a bit more enjoyable to use. I do
understand if the use of analytics bothers you. Please make an issue in the
tracker, or if someone already has, comment accordingly. If there are enough
people using this site that honestly care, I might consider removing the
analytics while I do further research.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ThothBackup - Part 1</title>
   <link href="http://zyradyl.github.io/2018/11/28/ThothBackup-Part-1/"/>
   <updated>2018-11-28T16:30:00-06:00</updated>
   <id>http://zyradyl.github.io/2018/11/28/ThothBackup-Part-1</id>
   <content type="html">&lt;p&gt;As many people may have guessed, this backup system very quickly got much larger
than I initially expected. Because of the size of the backup project, the number
of people interested, and how quickly things are changing along the way, I’ve
decided to approach this project in a new way.&lt;/p&gt;

&lt;p&gt;In the sidebar to the left you will notice there is a new link to a “Projects”
directory. Here you will be able to find all my larger works. The project is
now called ThothBackup, and what follows is a list of things I have learned
along the way. All of this data will be consolidated and entered in a more
coherent fashion into the project pages, so keep an eye out for those to update.&lt;/p&gt;

&lt;p&gt;But for now, we have a lot of ground to cover, so let’s get to work.&lt;/p&gt;

&lt;h4 id=&quot;part-1---cross-platform-is-hard&quot;&gt;Part 1 - Cross Platform is hard&lt;/h4&gt;

&lt;p&gt;One of the biggest parts of the backup project was its ability to be
cross-platform. I want the system to be easy enough to use that anyone and
everyone could grab a client, get it configured, and get going. To facilitate
this, the initial idea was to use tools that were built into the operating
system. On Linux and macOS this is easy enough, as &lt;code class=&quot;highlighter-rouge&quot;&gt;rsync&lt;/code&gt; is installed on most
distributions by default, and if it isn’t installed it is just a quick package
manager installation away.&lt;/p&gt;

&lt;p&gt;Then however, entered Windows. Initially I assumed that it would be easy to use
with windows as well. After all, &lt;a href=&quot;https://www.rsync.net/resources/howto/windows_rsync.html&quot;&gt;rsync.net&lt;/a&gt; has a nice little guide
explaining how to set it up. However, their client can detect when you’re not
using rsync.net servers (which is totally fair, there’s no hate from me on that)
and limits using the program to 30 days. The other alternative is cwRsync, which
was initially freeware, but has since changed to being a paid product. Obviously
asking someone to play for a program to even be able to start to use the backup
isn’t a great selling point.&lt;/p&gt;

&lt;p&gt;The first idea that I had was to write something on my own. Maybe have shell
scripts on all platforms check for required code and fetch anything that is
needed. However, shell scripts are hard for many people to debug, and the sight
of a command prompt can strike fear into the hearts of many windows users.&lt;/p&gt;

&lt;p&gt;The second iteration of the idea was to write something in Python. However at
that point the client is becoming a software project in its own right, and I
didn’t start this to develop software, I started it because I wanted to set up
a neat little backup service for my friends and family.&lt;/p&gt;

&lt;p&gt;Thankfully, there are many other software suites that are both cross platform
and useful for this task. We ended up going with &lt;a href=&quot;https://syncthing.net/&quot;&gt;SyncThing&lt;/a&gt;. SyncThing is
an open-source (MPL2, which is a permissive form of copyleft) synchronization
library that is cross-platform and written in Go. I’m a huge fan of Go even
though I don’t actually write it myself, as it is a fantastic language for
exactly this type of thing. Even better, SyncThing comes with easy to use and
easy to understand GUIs, and is capable of NAT and Firewall punching via relays,
and makes device configuration dependent on acceptance from both the server
and the client. The protocol it uses is open source, and based on the usage
reports at least one person is using it on 30 million files with 2,000 peers.
Last, but most certainly not least, traffic is encrypted with 128 bit AES,
and the protocol maintains perfect forward secrecy.&lt;/p&gt;

&lt;p&gt;All of this (and a whole lot more, it really is an awesome bit of software)
makes SyncThing perfect for our use case. This may not always remain the case,
but it gives me somewhere to start. Even if we end up moving beyond SyncThing in
the future, you really should give it a look. It is a phenomenal piece of
software.&lt;/p&gt;

&lt;h4 id=&quot;part-2---changes-in-sync-methods&quot;&gt;Part 2 - Changes in Sync Methods&lt;/h4&gt;

&lt;p&gt;As hinted above, the original plan to synchronize systems wasn’t going to work
without more work than I was willing to put in to a single component of the
system. Once we threw out the initial way the system was supposed to work, we
had to retool the way things worked on the operating system too.&lt;/p&gt;

&lt;p&gt;The original way the sync process was meant to work was that every user’s
operating system would get its own Server Account, and rsync or some other
synchronization system would be tunneled through SSH. I wasn’t sure if
authentication would be handled by system accounts or LDAP, because I never got
that far. But I did specifically pick the operating system (OpenSUSE) because
of that distribution’s system configuration manager (YaST).&lt;/p&gt;

&lt;p&gt;Now with the use of SyncThing, a daemon process would run under a single user,
and all clients would then connect to that daemon process which would then
write to disk using that daemon’s permission set. Thus, no need to worry about
ACLs or anything of the like. It was interesting to work with ACLs though. You
can see some of my old code if you browse through the commits history of the
ThothBackup GitHub repository.&lt;/p&gt;

&lt;h4 id=&quot;part-3---filesystem-considerations&quot;&gt;Part 3 - Filesystem Considerations&lt;/h4&gt;

&lt;p&gt;When I was testing the original synchronization strategy, I had everything being
deposited onto BTRFS subvolumes that were mounted with the &lt;code class=&quot;highlighter-rouge&quot;&gt;compress&lt;/code&gt; option. To
be entirely honest, I wasn’t that impressed with the way the compression was
working.&lt;/p&gt;

&lt;p&gt;In the new system, BTRFS subvolumes are still being used (User, System Name,
Operating System, Drive Name, Backup Client, Archive Client, etc) except now
the subvolumes are mounted with &lt;code class=&quot;highlighter-rouge&quot;&gt;compress-force&lt;/code&gt; option. Additionally, I have
learned about out-of-band BTRFS deduplication and plan to play around with that
at this stage in the project as well.&lt;/p&gt;

&lt;h4 id=&quot;part-4---operating-systems&quot;&gt;Part 4 - Operating Systems&lt;/h4&gt;

&lt;p&gt;I really, really like OpenSUSE. Like, a whole whole lot. It may very well be my
favorite binary distribution, and I’ve used quite a few. I think the whole way
it works is simply phenomenal, I like the company behind it, and it honestly
boils down to just that: I like it.&lt;/p&gt;

&lt;p&gt;But, after all the changes above I began to consider if I shouldn’t change
distributions. Originally I thought of changing to BSD, but I was concerned
about software availability. I know FreeBSD tends to have a very well maintained
ports collection, but I was still.. concerned. Most tools in this arena seem to
cater toward Linux, and if I was already changing multiple systems to avoid
having to write new software, did I really want to run the risk of needing to
write server side software?&lt;/p&gt;

&lt;p&gt;After much deliberation, I ended up settling on Debian Stable with backports.
The initial installation is extremely lean, and there is a truly massive amount
of documentation available for Debian. It paid off well too. The initial install
of Debian stable clocked in at 60MB of ram used, where as OpenSUSE was running
around 200MB after reboot.&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;There is honestly still quite a bit more that needs to be discussed. One of the
most amusing things that the past week or so has taught me is that Sydney’s
computer is as good of a backup test as a normal single family household. His
System has 4 drives, over one million files, a quarter of a million directories,
and about a terabyte of &lt;em&gt;used&lt;/em&gt; storage on it. Combining his single computer
with my mac and a windows virtual machine, and we have as much testing as we
could need.&lt;/p&gt;

&lt;h4 id=&quot;notes&quot;&gt;Notes&lt;/h4&gt;
&lt;p&gt;I’m going to start including a little section at the bottom of each post to
remind me what I need to work on. Hopefully having this publicly viewable will
encourage me to actually follow through on writing more than one blog post every
18 days.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;stage 1 project page&lt;/li&gt;
  &lt;li&gt;stage 2 project page&lt;/li&gt;
  &lt;li&gt;talk about security improvements that can be done&lt;/li&gt;
  &lt;li&gt;rewrite the server side new client script&lt;/li&gt;
  &lt;li&gt;talk about specific SyncThing configuration options used&lt;/li&gt;
  &lt;li&gt;write utility script to keep server config files up to date in git&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Yet Another Backup System - Part 1</title>
   <link href="http://zyradyl.github.io/2018/11/10/Yet-Another-Backup-System-Part-1/"/>
   <updated>2018-11-10T12:00:00-06:00</updated>
   <id>http://zyradyl.github.io/2018/11/10/Yet-Another-Backup-System-Part-1</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I’ve been wanting to get back into blogging, or at least writing more
consistently, and I’ve also been looking for projects to work on. One of my
favorite things to do, oddly enough, is to work on systems that will manage
large amounts of data. So, I’ve decided to start a new project.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts detailing the creation of a multi-tiered
backup system for me, some of my friends, and potentially (but not likely)
others as well. I will warn people ahead of time that I’m not building any
hardware that could be called “resilient,” instead I’m using things that I
already have around the house, plus a few extra cheap purchases, and a lot
of software to make things work.&lt;/p&gt;

&lt;p&gt;Speaking of software, almost everything used will be cross-platform and open
source. I will likely be writing a lot of “glue,” in the form of shell scripts
or potentially even some ruby. I will also be trying to focus on keeping things
secure on untrusted servers, and secure in transit, to sometimes ridiculous
levels. That being said, this is &lt;strong&gt;NOT&lt;/strong&gt; a zero-knowledge system. Since it is
for me, family, and friends, they are made aware of this ahead of time. In time,
it is possible that this setup will grow and evolve to make a zero-knowledge
system that functions similarly to this initial design idea, but that’s far
down the line.&lt;/p&gt;

&lt;p&gt;None of these blog posts will likely be as refined as my Icinga2 tutorials. One
of the things I am trying to do is just get myself into the habit of documenting
my projects. If I keep having to revise, edit, and source, I’ll eventually give
up. So if there is anything you are interested in learning more about, Google
will likely be your best friend.&lt;/p&gt;

&lt;p&gt;Essentially, these are meant to explain how I built the system, and why I did
what I did. Sometimes that may be as simple as “because I like them more” and
other times it may have to do with more in depth research.&lt;/p&gt;

&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;/h2&gt;

&lt;p&gt;The server will exist in an environment with a battery backup system provided
for the server, any data drives, and the core router that the server will be
connected to. Connection to the internet is provided with a 100mbps symmetrical
fiber connection that can be upgraded instantly to 1Gbps symmetrical if needed.
Connections between the server and the fiber node is provided via CAT-7A
cabling. Routing is handled by a Ubiquiti EdgeRouter Lite, with 3gbps line rate
and one million packet per second routing. IPv6 and IPv4 are supported.&lt;/p&gt;

&lt;p&gt;Eventually the router will be able to provide routes both to the public internet
and to the dn42 darknet and Tor services.&lt;/p&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware&lt;/h2&gt;

&lt;p&gt;The hardware isn’t exactly great, but it is the best that can be done at this
time. The server consists of a Mac Mini Model 2,1 made circa early 2007. It
contains a dual core Intel Core2 Duo processor running at 2.0 GHz.
&lt;code class=&quot;highlighter-rouge&quot;&gt;/proc/cpuinfo&lt;/code&gt; is provided here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;processor&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vendor_id&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GenuineIntel&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;family&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;           &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Intel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Core&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CPU&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;T7200&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2.00&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GHz&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stepping&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;microcode&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0xd1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MHz&lt;/span&gt;         &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2000.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4096&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KB&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;physical&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;siblings&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;         &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cores&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;apicid&lt;/span&gt;          &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;initial&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apicid&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fpu&lt;/span&gt;             &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fpu_exception&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpuid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wp&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;           &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vme&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;de&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tsc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pae&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx8&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apic&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtrr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pge&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mca&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pse36&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clflush&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acpi&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mmx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fxsr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sse2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ht&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pbe&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syscall&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constant_tsc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arch_perfmon&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pebs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rep_good&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nopl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpuid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aperfmperf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pni&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtes64&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ds_cpl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tm2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssse3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx16&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xtpr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pdcm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lahf_lm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pti&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpr_shadow&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtherm&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bugs&lt;/span&gt;            &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpu_meltdown&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spectre_v1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spectre_v2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spec_store_bypass&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1tf&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bogomips&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3999.68&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clflush&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;64&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cache_alignment&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;64&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;address&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;36&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;physical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;48&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;virtual&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;management&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;processor&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vendor_id&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GenuineIntel&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;family&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;           &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Intel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Core&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CPU&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;T7200&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2.00&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GHz&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stepping&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;microcode&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0xd1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MHz&lt;/span&gt;         &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1000.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4096&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KB&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;physical&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;siblings&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;         &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cores&lt;/span&gt;       &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;apicid&lt;/span&gt;          &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;initial&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apicid&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fpu&lt;/span&gt;             &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fpu_exception&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cpuid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wp&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;           &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fpu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vme&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;de&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tsc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pae&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mce&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx8&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apic&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtrr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pge&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mca&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pse36&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clflush&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acpi&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mmx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fxsr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sse&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sse2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ht&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pbe&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syscall&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constant_tsc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arch_perfmon&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pebs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rep_good&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nopl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpuid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aperfmperf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pni&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtes64&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ds_cpl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmx&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tm2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssse3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cx16&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xtpr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pdcm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lahf_lm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pti&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpr_shadow&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtherm&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bugs&lt;/span&gt;            &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpu_meltdown&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spectre_v1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spectre_v2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spec_store_bypass&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1tf&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bogomips&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3999.68&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clflush&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;64&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cache_alignment&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;64&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;address&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt;   &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;36&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;physical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;48&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;virtual&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;management&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The server has been modified substansially from it’s factory state. The sound
card has been removed, as has the WiFi card and Bluetooth card. IR functionality
has been disconnected. Any extraneous wires have been removed. The original
160GB Hitachi hard drive has been replaced with a 240GB solid state drive from
OWC. The ram has been maxed out with 4GBs, again provided from OWC. The system
is not capable of addressing all 4GB despite being a 64-bit system due to
limitations imposed in the EFI. The CMOS battery was replaced, and the processor
was unseated, cleaned, and thermal material replaced. The fan was also replaced
to head off any potential issues caused by old age. You can see a picture of the
internals of the system below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/mac-mini.jpeg&quot; alt=&quot;Modified MacMini2,1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The system is connected to the router with a Fluke-Certified CAT-7A S/FTP dual
shielded cable. The interface it is connected to supports a 1Gbps full-duplex
connection.&lt;/p&gt;

&lt;p&gt;The primary data disk is a G-Disk USB-C 4TB external drive. This drive utilizes
a white label Western Digital Red WD40EFRX. It is connected to the Mac Mini via
an Anker PowerLine+ USB-C to USB-A nylon cable.&lt;/p&gt;

&lt;p&gt;The older Mac Mini’s would refuse to boot if the system could not detect that a
monitor was connected to the system. Apple claims that this is because the Mac
Mini was intended to be a true personal computer, which would generally prohibit
the system’s operation without a display device. Strangely, no such detection is
present to test for either a keyboard or a mouse.&lt;/p&gt;

&lt;p&gt;To bypass this issue, a DVI Dummy Plug is installed in the Mini’s DVI port. This
plug uses EDID to tell the system that a 1920x1200 capable display is connected.
This has also been reported to be useful in accelerating VNC Remote
Administration, however no GUI is present on the system.&lt;/p&gt;

&lt;h2 id=&quot;operating-system&quot;&gt;Operating System&lt;/h2&gt;

&lt;p&gt;The operating system used on the server is OpenSUSE Leap 15 x64. You may
remember that I mentioned above that the EFI used on the system is 32-bits.
This limitation is bypassed by formatting the internal server drive to utilize
a legacy MBR partition table. From this, GRUB2 loads in legacy mode, which is
32-bits. This provides the shim that allows the 32-bit system firmware to load
a 64-bit operating system. Over 72 hours of stress testing revealed no
instability with the operating system booted via this method.&lt;/p&gt;

&lt;p&gt;OpenSUSE was installed using the Server profile. Originally the
Transactional-Server profile was used, however the early morning reboots
could cause issues, and the installation did not seem to play well with LVM.&lt;/p&gt;

&lt;h2 id=&quot;hardware-stress-testing&quot;&gt;Hardware Stress Testing&lt;/h2&gt;

&lt;p&gt;The internal solid state drive was tested via an OpenSUSE Live distribution. The
computer was first suspended and resumed to bypass the system’s EFI’s proclivity
for freezing the internal drive’s security interface. Once the interface was
unlocked, &lt;code class=&quot;highlighter-rouge&quot;&gt;smartctl&lt;/code&gt; was used to trigger the drives internal SMART short test,
conveyance test, and extended test, in that order. All results were nominal.
The internal drive was then wiped via &lt;code class=&quot;highlighter-rouge&quot;&gt;hdparm&lt;/code&gt; by sending an ATA Enhanced Secure
Erase Command with a non-null password.&lt;/p&gt;

&lt;p&gt;A quick installation of OpenSUSE Leap 15 was then performed.&lt;/p&gt;

&lt;p&gt;The hardware was then tested from inside the new installation utilizing the
Stress-NG test suite for over 24 hours. During this time, four virtual memory
test workers iterated over 100% of the available memory to force swapping as
well as check for ram errors. Two CPU test workers were ran on the aggressive
settings, and an IO worker was deployed to check the SSD interface. All systems
passed the stress test perfectly, with benchmarks appropriate for a system from
2007.&lt;/p&gt;

&lt;p&gt;The external drive was then connected to the system. A battery of tests were
performed. First, the drive was tested via &lt;code class=&quot;highlighter-rouge&quot;&gt;smartctl&lt;/code&gt;. As the drive is connected
via USB, the &lt;code class=&quot;highlighter-rouge&quot;&gt;-d sat,16&lt;/code&gt; option was passed to allow communication directly with
the USB-SATA bridge. The drive was tested out of the box using the SMART short
self test, conveyance test, and extended self test. This first round of testing
took 10 hours to execute. After the inital round of tests, the SMART attributes
table and its corresponding values was recorded.&lt;/p&gt;

&lt;p&gt;Then the &lt;code class=&quot;highlighter-rouge&quot;&gt;badblocks&lt;/code&gt; command was ran to write to and read from every sector on
the drive. The full command was
&lt;code class=&quot;highlighter-rouge&quot;&gt;badblocks -wsv -t random -b 4096 -c 4096 -p 4 -o /root/sdb.log /dev/sdb&lt;/code&gt;. This
performed four read-write-verify passes. Each pass took approximately 50 hours
to complete.&lt;/p&gt;

&lt;p&gt;Once the &lt;code class=&quot;highlighter-rouge&quot;&gt;badblocks&lt;/code&gt; battery was complete, &lt;code class=&quot;highlighter-rouge&quot;&gt;smartctl&lt;/code&gt; was once again used to
run the same testing sequence of short, conveyance, and extended self tests. The
attribute table was once again dumped, it’s values recorded, and the values then
compared to those from the table dumped prior to running &lt;code class=&quot;highlighter-rouge&quot;&gt;badblocks&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;As these tests are actually still in progress, the results are not yet
definitive.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This first post simply outlines the primary variables in this project. Hardware,
operating system, and environment should all be assumed to remain static for
the remaining project entries. Should anything change, I will likely update this
entry versus notating it in the respective entry where the change was made. It
will be made clear that this entry has been edited, should that come to pass.&lt;/p&gt;

&lt;p&gt;Extensive testing was performed to detect any chance of infant-mortality in the
hardware. I have previously had negative experiences utilizing hardware right
out of the box.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Collegi Pixelmon - Developer Log</title>
   <link href="http://zyradyl.github.io/2016/10/14/Collegi-Backup-Work-Notes/"/>
   <updated>2016-10-14T03:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2016/10/14/Collegi-Backup-Work-Notes</id>
   <content type="html">&lt;p&gt;The following post is an extreme rough draft. In fact, it isn’t even actually a
post. These are my development notes from my refactoring of the collegi data
infrastructure. As such, they’re arranged in no real sensible order besides
having been written chronologically. Additionally, these have not been
proofread, grammar checked, copyedited, or spell checked, as i write them in an
IDE and not an actual text editor. As such, please don’t judge my writing
ability off of them. More importantly, however, these do not have the
standardized links that i provide to new concepts or commands in my blog posts,
as embedding links to things I already know or have access to in a developer log
that on average no one else sees just seems silly.&lt;/p&gt;

&lt;p&gt;So, if you have questions, use google, and expect these to be updated over time.&lt;/p&gt;

&lt;p&gt;The logs as of this posting run from 10/13/2016 to 10/16/2016, so over three
days of work. There is a -LOT- more to be done.&lt;/p&gt;

&lt;p&gt;They are broken down into the following format. Each list is a set of specific
actions I took, and sometimes the list ends up with notes in it because, again,
no one generally sees these, but under the task list is the space reserved for
notes on the above list. Then a new task list is declared, then notes, then
tasks, and so on and so forth. Generally each new task heading would signify
a new blog post, talking about the tasks and the notes, so keep that in mind.&lt;/p&gt;

&lt;p&gt;These were requested by Kan, a player on our server. Enjoy!&lt;/p&gt;

&lt;h3 id=&quot;tasks&quot;&gt;Tasks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Made a backup of the repository as it stood on 2016-10-13 in the event
anything breaks too badly during this.&lt;/li&gt;
  &lt;li&gt;Removed all existing submodules from the &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; repository. Committed the
removal.&lt;/li&gt;
  &lt;li&gt;Ran the previous backup script to make sure that 10/13 was backed up. This
included new additions to git annex.&lt;/li&gt;
  &lt;li&gt;Forced git annex to drop the old SHA256E key-value backend files that were
made obsolete by the conversion to SHA512E key-value backend.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 1:&lt;/strong&gt; During this time, and while watching the way the version 1.0
backup script ran, I noticed there is a significant performance penalty for
moving the location of the local mirror. Borg uses the entire path as the file
name, so any deviation in the path spec causes it to treat the files as brand
new. Note that this does not cause any issues with de-duplication, but the
process of adding these files causes a massive performance hit. This made me
start thinking about including the local mirror in the git annex so that as
long as the annex was kept in tact in regards to metadata, the paths would
remain the same as all additions to Borg would take place from the same root
directory.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The problem with this would be the fact that annex keeps everything as
symlinks. As such, I am looking into the unlock feature of version six
repositories.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 2:&lt;/strong&gt; Dropping unused from a local area goes -much- faster than
dropping from remote. Who knew, right?&lt;/em&gt; :tongue:&lt;/p&gt;

&lt;h3 id=&quot;tasks-1&quot;&gt;Tasks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git-Annex&lt;/code&gt; drop completed, but Finder isn’t showing a reduction in used drive
space, but I think this is more an error on the side of finder than something
with git annex, as &lt;code class=&quot;highlighter-rouge&quot;&gt;du -h&lt;/code&gt; showed the directory was down to the size it should
have been. Once I manage to get this finder thing figured out, I’ll move on to
the next part.&lt;/li&gt;
  &lt;li&gt;Finder is taking too bloody long to figure its shit out, so I moved on to the
next step in cleaning up the repository. I’m rewriting the commit history
to completely remove files I don’t need from the actual &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; repo. In theory
this shouldn’t touch &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt; at all, but that remains to be seen.&lt;/li&gt;
  &lt;li&gt;Ran BFG Repo Cleaner on the following directories and files:
    &lt;ul&gt;
      &lt;li&gt;collegi.web&lt;/li&gt;
      &lt;li&gt;collegi.pack&lt;/li&gt;
      &lt;li&gt;collegi.git&lt;/li&gt;
      &lt;li&gt;.DS_Store&lt;/li&gt;
      &lt;li&gt;.gitmodules&lt;/li&gt;
      &lt;li&gt;collegi.logs (Just for a moment, and we made backups.)&lt;/li&gt;
      &lt;li&gt;collegi.configs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ran filter branch to purge any empty commits left after the above.&lt;/li&gt;
  &lt;li&gt;Expired original ref-logs, repacked archive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 3:&lt;/strong&gt; At this point we had gone from 230 commits to 102 commits. We were
also left with the original envisioning of what this repo would be, which was
a simple git annex to push files to Backblaze b2 from the Borg repository. Now
to verify that all of our data is still 100% ok.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;tasks-2&quot;&gt;Tasks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Ran &lt;code class=&quot;highlighter-rouge&quot;&gt;git fsck&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Ran &lt;code class=&quot;highlighter-rouge&quot;&gt;git annex fsck&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 4:&lt;/strong&gt; Wow this is going to take a long fucking time. Who woulda thunk
it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 5:&lt;/strong&gt; So apparently the current version of &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt; is using the old
mixed hashing method, which is a format that “we would like to stop using”
according to the wiki. Might need to migrate. Need to figure out how.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 6:&lt;/strong&gt; From the wiki: “Initial benchmarks suggest that going from xX/yY/KEY/OBJ to xX/yY/OBJ directories would improve speed 3x.” It’s worth
migrating.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;tasks-3&quot;&gt;Tasks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Run &lt;code class=&quot;highlighter-rouge&quot;&gt;git annex uninit&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Reading through the &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex-init&lt;/code&gt; man page to see what else we should
change now since we’re already migrating. Post Uninit we’re going to have to
run a full borg data consistancy check.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 7:&lt;/strong&gt; Ugh. The document I found was actually an theoretical one, and
while it is true that &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt; does use the new hashing format in bare
repositories there is no actual way to move to the new one in a regular repo.
So I am running an &lt;code class=&quot;highlighter-rouge&quot;&gt;uninit&lt;/code&gt; for basically no reason. The only good thing about
this that I can think of is that I will be able to reform the final &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt;
repo in a much saner fashion. The bad news is that I have lost the log files,
unless &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt; is going to bring those back for me. I am annoyed.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 8:&lt;/strong&gt; Good news! I just remembered that I had made a &lt;code class=&quot;highlighter-rouge&quot;&gt;rsyn&lt;/code&gt;ced backup
of the repository before I started fucking with it. So I didn’t actually lose
the log files, I just went ahead and pulled them out of the &lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt;
backup.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;tasks-4&quot;&gt;Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;After the git annex had uninitialized, I decided that if I was going to do
this whole damn thing over again I was going to do it right.&lt;/li&gt;
  &lt;li&gt;Started a new &lt;code class=&quot;highlighter-rouge&quot;&gt;borg&lt;/code&gt; repository in new-collegi. Pulled out contents from the
original &lt;code class=&quot;highlighter-rouge&quot;&gt;borg&lt;/code&gt; repository, using backups to restore any files that got hit in
the above clusterfuck, then recompressed with maximum LZMA compression.&lt;/li&gt;
  &lt;li&gt;During this period I also standardized how the &lt;code class=&quot;highlighter-rouge&quot;&gt;borg create&lt;/code&gt; paths would work.
The server would exist within a collegi.mirror directory, and the entire
directory would be added to &lt;code class=&quot;highlighter-rouge&quot;&gt;borg&lt;/code&gt; upon each run of the backup script. This
effectively means we never have to worry about the LZMA penalty discussed below
again after the first re-add, unless we do major server restructuring, because
paths will remain stable between commits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 9:&lt;/strong&gt; The initial speed penalty for using LZMA is absolutely jaw
dropping. One &lt;code class=&quot;highlighter-rouge&quot;&gt;borg create&lt;/code&gt; took eight hours to complete. Eight. However, I
quickly noticed that due to Borg’s de-duplication mechanism, the add times got
faster the more data I added, and gzip-9 to lzma-9 did actually yield some
improvement. It also reduces the incentive for me to do this fucking disaster
again, because of how much it absolutely fucking sucks.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 10:&lt;/strong&gt; As an example of what I mean by the above, the initial adding of
1.8.9 took six hours with LZMA-9. When the map was changed from NewSeed over to
Collegi, it took another four hours just to update the paths and what not, even
though the data hadn’t updated, just the paths have changed. (This is indicated
by the fact that the total repository size barely increased, all the size that
changed could be explained by new metadata.) However, when the paths are kept
the same, adding 100GB of data takes 13 to 15 minutes. So, the benefit of
LZMA-9 is worth the initial startup, imho.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 11:&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;borg extract&lt;/code&gt;ing from the GZIP-9 archives takes about 40
minutes, and that’s from highly de-duplicated and GZIP-9 archives. What this
means is that pulling from an lzma-9 is probably going to take about an hour,
depending on just how de-duplicated the archive is (as in, how many different
chunk files contain parts needed to reassemble the original content).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 12:&lt;/strong&gt; Have hit the series of backups where things have moved into the
Users path, and I’m restructuring them. It made me think about how I will handle
the mirror directory in the future. I think I am going to do a few new things
with respect to the new setup. The mirror directory will be a part of the
&lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt; repository, so there will be a new folder inside it called
&lt;code class=&quot;highlighter-rouge&quot;&gt;collegi.mirror&lt;/code&gt; or something similar, and then I can move the new backup
script to be ran from the root directory, which will be beneficial. That way
everything is neatly packaged. the issue becomes mirroring this, because
uploading that much constantly changing data to backblaze would be literally
stupid, and not at all within our budget. What I will likely do is initialize
a “bare repository” on my time machine drive, and mirror the entirity of the
&lt;code class=&quot;highlighter-rouge&quot;&gt;git-annex&lt;/code&gt; repository to that.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;mandatory-break-notes&quot;&gt;Mandatory Break Notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;You need to run borg info to make sure the latest creation thingy is the
proper size, and a borg check might not be a bad idea either as you fell asleep
and closed the mac during work on the repo.&lt;/li&gt;
  &lt;li&gt;Cleaned the time machine volume of the repeated backups of the new repository
because it doesn’t make any sense to have 20 versions of it.&lt;/li&gt;
  &lt;li&gt;Moved the repo to the time machine drive as temporary storage using rsync.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tasks-5&quot;&gt;Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Restarted the transfer process starting on the 8th of October&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 13:&lt;/strong&gt; Not a huge shock but running some of these commands across USB
2.0 can add anywhere from 10 to 30 minutes. Doing them cross device gets even
worse, with some transactions taking almost an hour.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 14:&lt;/strong&gt; I’ve been going back and forth on what filesystem I would like
to deploy since I am redoing the collegi drive as a whole. Now the interesting
thing to note here is that by the time I get this thing fully ready to deploy,
the drive I have here may not be the drive it ends up on, but this is as good
of a testbed as any. I’m really thinking I will go with apfs. Most of the
gripes I have with it are easily resolved through borg and git annex.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 15:&lt;/strong&gt; In a highly amusing turn of events, it is bigger in lzma 9 than
it was with gzip 9. weird.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notes 16:&lt;/strong&gt; While it would likely be prudent to go back to the previous
compression method, the benefits that I have made to the directory structure
while redoing the borg repository are worth the few extra gigabytes of overhead
especially concerning with Backblaze B2 it barely costs a penny.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;tasks-6&quot;&gt;Tasks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Use JHFSX for the new drive. I would have really liked to use APFS but I am
still worried about the data loss considering there is almost a year till it
will ship. JHFSX is reasonable enough for right now, while still being safe to
unplug.&lt;/li&gt;
  &lt;li&gt;I went round and round on using encryption on the new drive. did it.&lt;/li&gt;
  &lt;li&gt;using rsync to bring the data to its final resting location.&lt;/li&gt;
  &lt;li&gt;OK started setting things up&lt;/li&gt;
  &lt;li&gt;Defined gitlab as the metadata backup again&lt;/li&gt;
  &lt;li&gt;created a bare repository on skaia&lt;/li&gt;
  &lt;li&gt;set up prefered content so skaia requires everything in the main repo&lt;/li&gt;
  &lt;li&gt;set the main repo to require a –force to drop content via preferred content&lt;/li&gt;
  &lt;li&gt;Set the backend to SHA512E&lt;/li&gt;
  &lt;li&gt;began the long process of adding the data to the git-annex&lt;/li&gt;
  &lt;li&gt;Set up bin directory to not be tracked by git-annex but instead by git&lt;/li&gt;
  &lt;li&gt;added backblaze remote, not encrypted, with a proper prefix&lt;/li&gt;
  &lt;li&gt;started to sync to backblaze&lt;/li&gt;
  &lt;li&gt;noticed an issue with how the sync was going to gitlab, will correct.
    &lt;ul&gt;
      &lt;li&gt;corrected the issue&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Collegi Pixelmon - Backup System Part 2</title>
   <link href="http://zyradyl.github.io/2016/10/13/Collegi-Backup-Part-2/"/>
   <updated>2016-10-13T12:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2016/10/13/Collegi-Backup-Part-2</id>
   <content type="html">&lt;p&gt;What was originally intended to be a one off blog post may become my new source
of material for the coming weeks. After utilizing &lt;a href=&quot;https://github.com/borgbackup/borg&quot; title=&quot;BorgBackup&quot;&gt;BorgBackup&lt;/a&gt; and
&lt;a href=&quot;http://git-annex.branchable.com/&quot; title=&quot;Git-Annex&quot;&gt;git-annex&lt;/a&gt; to backup what has now grown to almost &lt;a href=&quot;https://twitter.com/Zyradyl/status/786205897810780160&quot; title=&quot;Zyradyl's Twitter&quot;&gt;2.5 Terabytes&lt;/a&gt; of
data, I began to wonder what other ways I could put git-annex to use for us here
at &lt;a href=&quot;http://collegi.enjin.com&quot; title=&quot;Collegi Pixelmon Main Website&quot;&gt;Collegi&lt;/a&gt;. We already use various &lt;a href=&quot;https://gitlab.com/&quot; title=&quot;GitLab&quot;&gt;GitLab&lt;/a&gt; repositories to manage
different facets of the project, and I began to wonder if there wouldn’t be some
way to use git-annex to completely unify those repositories and distribute their
information as needed.&lt;/p&gt;

&lt;p&gt;This started as a brief foray into &lt;a href=&quot;https://medium.com/@porteneuve/mastering-git-submodules-34c65e940407#.24xm3mdlt&quot; title=&quot;Mastering Git Submodules&quot;&gt;git submodules&lt;/a&gt; which, while allowing me
to consolidate data &lt;strong&gt;locally&lt;/strong&gt;, does nothing in helping me to properly
redistribute that data to various locations. The only way that it would be
possible to do such a thing would be to take all the various git repositories
that Collegi utilizes, which currently is sitting at &lt;a href=&quot;https://gitlab.com/groups/collegi&quot; title=&quot;GitLab: Collegi Group&quot;&gt;six total&lt;/a&gt;, including
the git-annex metadata repository (which isn’t publicly visible), and merge them
into one master repository through the use of &lt;a href=&quot;https://medium.com/@porteneuve/mastering-git-subtrees-943d29a798ec#.r5p4ozyfm&quot; title=&quot;Mastering Git Subtrees&quot;&gt;git subtrees&lt;/a&gt;. This would
allow me to still have multiple repositories for ease of project management, but
all those repositories would be pulled down, daily, to a local “master”
git-annex repository and merged into it.&lt;/p&gt;

&lt;p&gt;Once this was done, the use of git annex’s &lt;a href=&quot;https://git-annex.branchable.com/git-annex-preferred-content/&quot; title=&quot;Git Annex: Preferred Content Manual Page&quot;&gt;preferred content&lt;/a&gt; system would
allow me to decide what data needed to be sent to which remote. This would let
me back up some information to one remote, and other information to another.
As an added bonus, the use of git subtrees would even allow me to push changes
back upstream, and all of it would be centralized.&lt;/p&gt;

&lt;p&gt;In the future, this would allow us to push very specific data to specific team
members, who would then modify the data, which would be pulled back down on the
next git-annex sync, we would see changes needing to be pushed upstream had been
made, unlock those files, then use git subtree to push them back to their
remotes. That’s the theory at least. As far as I am aware, either no one has
done this before, no one who has done this before has lived to tell the tale, or
no one who has done this before has blogged about their experiences in doing so.&lt;/p&gt;

&lt;p&gt;That’s where this blog comes in. I’m currently in the process of making a
complete copy of the current root repository, which is still using git
submodules, and from there I can begin experimenting. Whether or not this works
remains to be seen, but it coincides neatly with a rewrite of the
&lt;a href=&quot;https://gitlab.com/collegi/collegi-backup-automation&quot; title=&quot;GitLab: Collegi Group - Backup Automation&quot;&gt;backup script&lt;/a&gt; to update it to Google Shell Style Guidelines, which means
I can build the script around the new repository layout, and while doing so I
should be able to head off any unforeseen issues.&lt;/p&gt;

&lt;p&gt;It’s very likely that I am going to finish writing 2.0 of the script before
doing any of this crazy shit, but this post helps me to organize my thoughts.
Besides, it just means 3.0 will be that much more exciting when it drops.&lt;/p&gt;

&lt;p&gt;Stay tuned for more of my antics and adventures with making this absurd system
take shape, and turn into the omnipresent repository of every single facet of
a Minecraft community.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>The Collegi Pixelmon Server Backup System</title>
   <link href="http://zyradyl.github.io/2016/10/01/Collegi-Backup-System/"/>
   <updated>2016-10-01T22:30:00-05:00</updated>
   <id>http://zyradyl.github.io/2016/10/01/Collegi-Backup-System</id>
   <content type="html">&lt;p&gt;Wow, time flies. It has been almost a year since I last updated this blog,
including fixing some of the issues that Jekyll 3.0 introduced in my formatting.
Luckily, that could be fixed by just adding a few spaces. In the past year,
quite a bit has happened, but nothing quite so exciting as becoming a co-owner
and the head developer of a new Minecraft community called &lt;a href=&quot;http://collegi.enjin.com&quot; title=&quot;Collegi Pixelmon Main Website&quot;&gt;Collegi&lt;/a&gt;. Collegi
is a &lt;a href=&quot;http://pixelmonmod.com/&quot; title=&quot;Pixelmon Mod Main Website&quot;&gt;Pixelmon&lt;/a&gt; server, which means we have Pokemon right inside Minecraft.
However, we strive to make the server Minecraft with Pokemon, instead of Pokemon
in Minecraft. It’s a small difference, but one that we happen to find very
important. We want the survival aspect of the game to be front and centre.&lt;/p&gt;

&lt;p&gt;The server has become absolutely massive, with each downloaded snapshot running
about 100GB in size. (Note, that throughout this article I will be using the
SI standard GB, which is 10&lt;sup&gt;9&lt;/sup&gt;, versus the Gibibyte which is
2&lt;sup&gt;30&lt;/sup&gt;, how hard drive manufacturers were allowed to change the value of
a gigabyte is something I will never understand.)&lt;/p&gt;

&lt;p&gt;Now, with a 500GB flash drive on my MBP, I don’t really have the room to save
all of those snapshots, especially considering we have snapshots going back six
months, across three different major versions of Minecraft. In fact, completely
expanded, the current backup amount at the time of writing is 1.11TB.&lt;/p&gt;

&lt;p&gt;So, I began to search for a method of performing backups. I had some rather
strict requirements for these backups, that lead to the formulation of the
system I am going to discuss in this article.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Incremental FTP&lt;/li&gt;
  &lt;li&gt;Deduplication&lt;/li&gt;
  &lt;li&gt;Compression, and the ability to modify compression levels on the fly.&lt;/li&gt;
  &lt;li&gt;Checksumming to silently detect corruption.&lt;/li&gt;
  &lt;li&gt;Encryption&lt;/li&gt;
  &lt;li&gt;Tools need to be actively maintained and ubiquitous.&lt;/li&gt;
  &lt;li&gt;Able to sync repository with a remote source.&lt;/li&gt;
  &lt;li&gt;Cheap&lt;/li&gt;
  &lt;li&gt;Open source wherever possible.&lt;/li&gt;
  &lt;li&gt;Easy to access archived versions.&lt;/li&gt;
  &lt;li&gt;Must be able to be automated.
    &lt;ul&gt;
      &lt;li&gt;If not in setup, then in how it runs later.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-one---getting-the-data-off-the-server&quot;&gt;Step One - Getting the Data off the Server&lt;/h2&gt;
&lt;p&gt;We use a lovely company called &lt;a href=&quot;https://www.bisecthosting.com/&quot; title=&quot;BisectHosting&quot;&gt;BisectHosting&lt;/a&gt; to run our server. They
provide an extremely barebones budget package that gives us a large amount of
our most important specification: RAM. We can live without fancy support tickets
or SSD access if they offer us cheap RAM, which they do. Beyond that, however,
they also offer unlimited disk space, as long as that disk space goes towards
the server itself, so no keeping huge numbers of backups on the server.&lt;/p&gt;

&lt;p&gt;Now, they did offer a built in backup solution, but it only keeps the past seven
days available in a rolling fashion, and I really really like to keep backups.&lt;/p&gt;

&lt;p&gt;The only real gripe I have about BisectHosting is that they only allow the use
of FTP for accessing data on the Budget Server tier. Worse, they don’t even use
FTP over TLS, so the authentication is in plain text. However, I just change my
password weekly and it seems to work alright.&lt;/p&gt;

&lt;p&gt;The most important part of getting the data off the server is only getting the
new data, or the data that has changed. This requires using an FTP Client that
is able to sanely detect new data. Checksums aren’t available, but modification
date and file size work just as well.&lt;/p&gt;

&lt;p&gt;There were a large number of clients that I tried out over time. &lt;a href=&quot;https://filezilla-project.org/&quot; title=&quot;Filezilla Project&quot;&gt;Filezilla&lt;/a&gt;
was the first of those. It seemed to work alright for a time, except that when
you have a large amount of identical files (We have 15,824 files at the time of
this writing) it hangs. Now, it does come back eventually, but it’s still not
the best of features to have a client that hangs.&lt;/p&gt;

&lt;p&gt;The next one I tried was a Mac favourite known as &lt;a href=&quot;https://cyberduck.io/&quot; title=&quot;Cyberduck Website&quot;&gt;Cyberduck&lt;/a&gt;. I really liked
the interface for Cyberduck, but the first nail in its coffin was the inability
to perform a modification time comparison and a file size comparison during the
same remote to host sync. That meant it took two syncs to grab everything up to
date, and even then it didn’t always seem to take. During the time that I was
using Cyberduck, we had to restore from backup for some reason that is currently
eluding me, but when we did so we noticed that some recent changes on the map
hadn’t synced properly. Combine all of the above with the fact that from time to
time it would hang on downloads (I’m assuming from the absurd number of files)
and that wasn’t going to work.&lt;/p&gt;

&lt;p&gt;The final GUI client that I tried was called &lt;a href=&quot;https://www.panic.com/transmit/&quot; title=&quot;Transmit Website&quot;&gt;Transmit&lt;/a&gt;. I really, really
enjoyed using Transmit. It is a very polished interface, but first off it
isn’t free, or open source, so that invalidated two of the requirements.
However, if it worked well enough, I was willing to overlook the issues. Problem
was, it didn’t work well. I forget what happened at the moment, but I know that
it experienced similar hanging to Filezilla.&lt;/p&gt;

&lt;p&gt;Regardless, Transmit was the last GUI based client that I tried. It took me a
bit to realize, but if I used a GUI client there was a very minimal chance that
I would be able to automate the download.&lt;/p&gt;

&lt;p&gt;That left command line tools, which after I found &lt;a href=&quot;http://lftp.yar.ru/&quot; title=&quot;LFTP Homepage&quot;&gt;LFTP&lt;/a&gt; I kicked myself for
not looking into first. In addition to being an open source tool, LFTP has the
ability to perform multithreaded downloads, which isn’t common in command line
clients. Furthermore, it was able to compare both modification time and file
size simultaneously, reducing the sync operations needed back to one. It is
actively maintained, available in &lt;a href=&quot;http://brew.sh/&quot; title=&quot;Homebrew Website&quot;&gt;Homebrew&lt;/a&gt; (though, at the time of writing
 it has been moved into the boneyard), written in C, and very easily scriptable.
You can call commands that would normally have to be ran from inside the FTP
client directly from the command line invokation of LFTP. It handled our data
quantity flawlessly, and easily worked through the large amount of files, though
it can take quite a while to parse our biggest directories. At the time of
writing, that directory is the map data repository for our main world, which has
12,567 items clocking in at 88.15GB. It takes between two and five minutes for
LFTP to parse the directory, which considering all the other benefits is fine
by me.&lt;/p&gt;

&lt;p&gt;Our remote to local command utilizes the LFTP mirror function, and from within
the client, looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mirror -nvpe -P 5 / ~/Development/Collegi/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-two---convert-the-data-to-an-archive-repository&quot;&gt;Step Two - Convert the Data to an Archive Repository&lt;/h2&gt;
&lt;p&gt;When you are talking about a server that a full backup runs 100GB, and you want
to perform daily backups at minimum, it becomes absurd to think that you could
run a full backup every day. However, the notion of completely incremental
backups is far too fragile. If a single incremental backup is corrupted, every
backup after it is invalid. More than that, to access the data that was on the
server at the time the incremental was taken would require replaying every
incremental up to that point.&lt;/p&gt;

&lt;p&gt;The first solution I tried for this problem was to use &lt;a href=&quot;https://en.wikipedia.org/wiki/ZFS&quot; title=&quot;ZFS on Wikipedia&quot;&gt;ZFS&lt;/a&gt;. ZFS solves
almost every problem that we have by turning on deduplication and compression,
running it on top of Apple’s &lt;a href=&quot;https://en.wikipedia.org/wiki/FileVault&quot; title=&quot;FileVault on Wikipedia&quot;&gt;FileVault&lt;/a&gt;, and utilizing &lt;a href=&quot;https://docs.oracle.com/cd/E23824_01/html/821-1448/gbciq.html&quot; title=&quot;Oracle's Documentation on ZFS Snapshots&quot;&gt;snapshots&lt;/a&gt;. The
snapshots are complete moments in time and can be mounted, and they only take
up as much space as the unique data for that snapshot. Using ZFS Snapshots, the
1.10TB of data we had at that time was reduced to 127GB on disk. Perfect. The
problem becomes, however, offsite replication.&lt;/p&gt;

&lt;p&gt;Now, it is true that by having a copy of the data on the server, one on my
MacBook, and one on an external drive here at the house, the [3-2-1 Backup][12]
rule is satisfied. However, three backups of the data is not sufficient for a
server that contains over six months of work. It’s reasonable that something
cataclysmic could happen and we’d be shit out of luck. We needed another offsite
location. The only such location that offers ZFS snapshot support is
&lt;a href=&quot;http://rsync.net/&quot; title=&quot;Rsync.Net Homepage&quot;&gt;Rsync.net&lt;/a&gt; which 100% violates the “Cheap” requirement mentioned above.
That’s not a knock on their service, Rsync.net provides an incredible service,
but for our particular use case it just wasn’t appropriate.&lt;/p&gt;

&lt;p&gt;So the hunt began for a deduplicating, compression based, encrypted backup
solution that stored the repository in standard files on a standard filesystem.
The final contenders were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using plain old &lt;a href=&quot;https://git-scm.com/&quot; title=&quot;Git SCM&quot;&gt;Git&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bup.github.io/&quot; title=&quot;Bup: It Backs things Up!&quot;&gt;BUP&lt;/a&gt; (As a side note, this client has the most adorable name for a
backup utility that I have ever seen. I love it.)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://attic-backup.org/&quot; title=&quot;Attic Backup&quot;&gt;Attic Backup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/borgbackup/borg&quot; title=&quot;BorgBackup&quot;&gt;BorgBackup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://git-annex.branchable.com/&quot; title=&quot;Git-Annex&quot;&gt;git-annex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was leaning very, very heavily toward BUP until I discovered BorgBackup. My
primary concerns with BUP was that it did not seem to be under active
development, and after over five years it still had not reached a stable 1.0.
Git would have been useful, but just like ZFS it would inevitably require a
“Smart Server” versus the presentation of just a dumb file-system.&lt;/p&gt;

&lt;p&gt;BorgBackup sold me almost immediately. It allowed you to mount snapshots and
view the filesystem as it was at that time, it offers multiple levels of
compression ranging from fast and decent to slow and incredible, and it has
checksumming on top of HMAC encryption. It’s worth noting at this time that
nothing on the server is really so urgent as to require encryption, as most of
the authentication is handled by Mojang, but I still prefer to encrypt things
wherever possible.&lt;/p&gt;

&lt;p&gt;It was under active development, it’s developers were active in the community
(I ended up speaking with the lead developer on twitter), and it was progressing
in a sane and stable fashion. As an added bonus, the release of 1.1 was to
provide the ability to repack already stored data, allowing us to potentially
add a heavier compression algorithm in the future and convert already stored
data over to it.&lt;/p&gt;

&lt;p&gt;The only downside to Borg was that at first glance it seemed to require a Smart
server, just like git would.&lt;/p&gt;

&lt;p&gt;Regardless, the system would work for now. If worst came to worst, I could
utilize something like &lt;a href=&quot;http://rclone.org/&quot; title=&quot;Rclone&quot;&gt;rclone&lt;/a&gt; to handle uploading to an offsite location.&lt;/p&gt;

&lt;p&gt;When everything was said and done, we had reduced the size of our 1.11TB backup
into a sane, usable 127GB.&lt;/p&gt;

&lt;p&gt;The current command that is used looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;borg create --chunker-params=10,23,16,4095 --compression zlib,9 --stats \
    --progress /Volumes/Collegi/collegi.repo::1.10.2-09292016 .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;step-three---offsite-replication&quot;&gt;Step Three - Offsite Replication&lt;/h2&gt;
&lt;p&gt;I could easily spend a very long time here discussing how I chose the cloud
provider I would inevitably use for this setup, but it really comes down to
the fact that I quite like the company, and their cloud offering has a very
complete API specification, and is dirt cheap. We went with &lt;a href=&quot;https://www.backblaze.com/b2/cloud-storage.html&quot; title=&quot;BackBlaze Cloud Storage&quot;&gt;BackBlaze B2&lt;/a&gt;.
I could, and probably will, easily write a whole separate post on how enthralled
I am with BackBlaze as a company, but more than that their $0.005/GB/Month price
is literally unbeatable. Even &lt;a href=&quot;https://aws.amazon.com/glacier/&quot; title=&quot;Amazon Glacier&quot;&gt;Amazon Glacier&lt;/a&gt; runs for $0.007/GB/Month and
they don’t offer live restoration. It’s cold storage as opposed to BackBlaze’s
live storage.&lt;/p&gt;

&lt;p&gt;The problem became this: How do I get the Borg repository to fully sync to B2,
but do so in such a way that if the local repository ever became damaged I could
pull back only the data that had been lost. This is what the
&lt;a href=&quot;http://borgbackup.readthedocs.io/en/stable/faq.html#can-i-copy-or-synchronize-my-repo-to-another-location&quot; title=&quot;Borg Documentation: Can I sync my Repository to another Location?&quot;&gt;documentation for Borg&lt;/a&gt; means when it mentions you should really think
about if mirroring best meets your needs, and for us it didn’t.&lt;/p&gt;

&lt;p&gt;Again though, B2 is just a storage provider, not a smart server. So how do I set
things up in this way? The answer became to use another tool that was almost
used for backup in the first place, Git-Annex. The only reason git-annex wasn’t
used for backup to begin with is that it doesn’t allow us to retain versioning
information. It just manages large files through git, which wouldn’t work.
What it would do, however, and do quite well, is to act as a layer between our
BorgBackup repository and the cloud.&lt;/p&gt;

&lt;p&gt;So, I stored the entire borg repository into git annex. Once this was done, I
used a &lt;a href=&quot;https://github.com/encryptio/git-annex-remote-b2&quot; title=&quot;Git-Annex B2 Backend&quot;&gt;plugin&lt;/a&gt; for git-annex to add support for a B2 content backend. Then,
the metadata information for the git repository gets synced to &lt;a href=&quot;https://gitlab.com/&quot; title=&quot;GitLab&quot;&gt;GitLab&lt;/a&gt;, and
the content is uploaded to B2.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The end result of this is that our 100GB server, as it stands at any day, is
mirrored in four separate locations. One on the host itself, one on the MBP
hard drive, one in the Borg Repository, and one on the BackBlaze B2 Cloud. More
than that though, we have a system that is easily automated via a simple shell
script, which after completing the initial setup (sending 20,000+ files to
Backblaze B2 can take a while), I will demonstrate here.&lt;/p&gt;

&lt;p&gt;Thank you so much for reading, I look forward to sharing more about the inner
workings of the Collegi Infrastructure as time permits.&lt;/p&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;
&lt;p&gt;I just recently completed an asciinema of the process. See below. Also note
that you can copy and paste commands from inside the video itself. Go ahead, try
it!&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/14pvurnnazr6res7f5u1yvg7u.js&quot; id=&quot;asciicast-14pvurnnazr6res7f5u1yvg7u&quot; async=&quot;&quot;&gt;&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>C++ GUI Libraries</title>
   <link href="http://zyradyl.github.io/2015/12/14/CPP-GUI-Library-Hell/"/>
   <updated>2015-12-14T14:30:00-06:00</updated>
   <id>http://zyradyl.github.io/2015/12/14/CPP-GUI-Library-Hell</id>
   <content type="html">&lt;p&gt;So, one of the many projects that I have been working on as of late is a game
engine for a small game studio called Near The Resolution. Really awesome
people to work for, lemme tell ya. They are allowing me to release the engine
under one of my crazy “No Rights Reserved” licenses. I have learned that one
of the unfortunate parts of game design is that if you’re not using C++ then
you are essentially doomed to having to rewrite the wheel. After a lot of
research, I settled on utilizing SFML as the basis of the game engine, because
it is released under zlib.&lt;/p&gt;

&lt;p&gt;One of the many, I suppose, &lt;em&gt;features&lt;/em&gt; of C++ is that a number of libraries are
available for solving things without having to rewrite the wheel. This makes me
personally uncomfortable on a number of levels, but that comes with
&lt;a href=&quot;http://warp.povusers.org/OpenLetters/ResponseToTorvalds.html&quot;&gt;“C Hacker Syndrome”&lt;/a&gt; which I definitely suffer from. Anyways, I decided
that I would attempt to put this.. &lt;em&gt;feature&lt;/em&gt; to my advantage, and find a GUI
library that works properly with SFML, is clean and simple, and is small.&lt;/p&gt;

&lt;p&gt;Turns out, this isn’t actually possible.&lt;/p&gt;

&lt;h3 id=&quot;sfgui&quot;&gt;&lt;a href=&quot;http://sfgui.sfml-dev.de&quot;&gt;SFGUI&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;SFGUI is the first thing I spent time working on. The problem with SFGUI is it
suffers from “There’s a fleck on the speck on the tail on the frog on the bump
on the branch on the log in the hole in the bottom of the sea” Syndrome. If you
want to make a button, for example.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Create the label.
m_label = sfg::Label::Create( &quot;Hello world!&quot; );

// Create a simple button and connect the click signal.
auto button = sfg::Button::Create( &quot;Greet SFGUI!&quot; );
button-&amp;gt;GetSignal( sfg::Widget::OnLeftClick ).Connect( std::bind( &amp;amp;HelloWorld::OnButtonClick, this ) );

// Create a vertical box layouter with 5 pixels spacing and add the label
// and button to it.
auto box = sfg::Box::Create( sfg::Box::Orientation::VERTICAL, 5.0f );
box-&amp;gt;Pack( m_label );
box-&amp;gt;Pack( button, false );

// Create a window and add the box layouter to it. Also set the window's title.
auto window = sfg::Window::Create();
window-&amp;gt;SetTitle( &quot;Hello world!&quot; );
window-&amp;gt;Add( box );

// Create a desktop and add the window to it.
sfg::Desktop desktop;
desktop.Add( window );

// We're not using SFML to render anything in this program, so reset OpenGL
// states. Otherwise we wouldn't see anything.
render_window.resetGLStates();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So you want a button. There’s a Label on the button in the box on the window
on the desktop in the SFML window on the openGL at the bottom of the code tree.
Needless to say, this lasted about five minutes in our source tree before I
broke down sobbing. Not to mention, the entire system renders in its own area
of the main SFML window. So when I tried to make an SFGUI button in the SFML
window on top of our background, yeah it didn’t go to well, and I don’t need
&lt;em&gt;another&lt;/em&gt; engine on top of the engine on top of this game engine. That’s just..
absurd. Not to mention the documentation sucks. I hate crappy documentation.&lt;/p&gt;

&lt;h3 id=&quot;tgui&quot;&gt;&lt;a href=&quot;https://tgui.eu&quot;&gt;TGUI&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;TGUI bills itself as the “Simple GUI” for SFML. So naturally, I was really
quite excited to try it out. Until I figured out it required configuration
files, and every button had to be an image or it wouldn’t render. So in theory,
you couldn’t have a completely transparent button, which is one of the things
that this game will require. Secondly, if you require a configuration syntax,
you are &lt;em&gt;not&lt;/em&gt; a simple library. Ever. All I want is to be able to call methods
that create a button.&lt;/p&gt;

&lt;h3 id=&quot;cegui&quot;&gt;&lt;a href=&quot;http://cegui.org.uk&quot;&gt;CEGUI&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Desperate times call for desperate measures. Or so I believed. Problem is,
CEGUI actually contains its own OpenGL renderer, which is redundant on top of
SFML. There is never a reason to have two OpenGL renderers, they will conflict.
Furthermore, CEGUI requires XML. Which is -not- a simple library. They claim
that this is to reduce the frustration of having to change your code to adjust
the GUI interface, but I would rather adjust my code than end up dependent on
XML files.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;At the end of the day, All I really need to do for this particular engine is
lay out a few menus and then a general presentation on top of those for the
in game content. So I used &lt;a href=&quot;http://www.bromeon.ch/libraries/thor/&quot;&gt;Thor&lt;/a&gt; to render shapes, and then I’m writing my
own logic on top of that. This gives me total control over the implementation,
and the logic of the GUI system. Maybe I’m strange in that I don’t mind having
to change values in my code and recompile, but I honestly prefer to do that.
Control over individual placement makes sense, and most of the time I define
my placement on top of positioning logic, so in those cases it makes more sense
to have it in the code than in an external file.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>An Actual Update</title>
   <link href="http://zyradyl.github.io/2015/12/02/An-Actual-Update/"/>
   <updated>2015-12-02T11:30:00-06:00</updated>
   <id>http://zyradyl.github.io/2015/12/02/An-Actual-Update</id>
   <content type="html">&lt;p&gt;So I recently purchased a new MacBook Pro, and in the time it took me to get the
device, add in all my new stuff, migrate everything over, establish a new
backup system, and contend with school work, this blog ended up becoming very
neglected. It didn’t help matters much when I logged on from Safari and
realised none of my fonts were working properly.&lt;/p&gt;

&lt;p&gt;I believe I have now solved the font issue (woff files vs woff2 files), and I
have completed all the setup of this system, including getting my ruby workflow
transitioned. So, in theory, we will be able to get back to the good stuff here
shortly.&lt;/p&gt;

&lt;p&gt;I have recently joined a small game development startup with some very talented
people, and was berated into using C++ for the game by the way this particular
industry works, so you can expect some future posts about my frustrations with
developing a cross platform game engine in C++.&lt;/p&gt;

&lt;p&gt;The goal, for now, is to do the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Write a post on backup software evaluation, and why I went with the software
I did go with.&lt;/li&gt;
  &lt;li&gt;Finish up the Icinga2 Tutorials that have driven quite a lot of traffic to
this page.&lt;/li&gt;
  &lt;li&gt;Write some stuff on game design.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we will see how that works out. As always, thanks for reading.&lt;/p&gt;

&lt;p&gt;P.S. I have decided that I don’t really like having analytics on this page,
so over the next few days I am going to root through the code and remove them.
Thank you for your patience.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Blog Cleanup</title>
   <link href="http://zyradyl.github.io/2015/09/07/Blog-Cleanup/"/>
   <updated>2015-09-07T21:04:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/09/07/Blog-Cleanup</id>
   <content type="html">&lt;p&gt;I just spent the past thirty minutes going back and removing a number of the
links that were in my posts up to this point. Nothing important, just links
to Wikipedia or Man Pages, but after speaking with a few friends, I realized
the links weren’t exactly as useful as I thought they were. In addition to this
my build time on Travis-CI had hit about four minutes and all I have to build
is about eight pages, so that’s absurd, and not really scalable. All the
important links have been left in, along with any interesting or amusing ones.&lt;/p&gt;

&lt;p&gt;If you are looking for the links, you can go into the github repository for
this site, and look for the commits prior to “Automatic Commit: cleaning up
links”.&lt;/p&gt;

&lt;p&gt;This also helps to maintain my sanity, so there’s that. Fifty links in one
article was causing me to worry quite a lot about dead links.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Icinga2 Tutorial Part 4 - Expanding Checks to SNMP</title>
   <link href="http://zyradyl.github.io/2015/09/07/Icinga2-Tutorial-Part-4/"/>
   <updated>2015-09-07T16:18:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/09/07/Icinga2-Tutorial-Part-4</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/10/Icinga2-Tutorial-Part-0/&quot;&gt;Icinga2 Tutorial: Part 0 - Network Monitoring for the Masses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/11/Icinga2-Tutorial-Part-1/&quot;&gt;Icinga2 Tutorial: Part 1 - Installation and Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/12/Icinga2-Tutorial-Part-2/&quot;&gt;Icinga2 Tutorial: Part 2 - Agent-Less Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/17/Icinga2-Tutorial-Part-3/&quot;&gt;Icinga2 Tutorial: Part 3 - Agent-Based Checks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;EDIT (2018/12/09):&lt;/strong&gt; &lt;em&gt;These guides haven’t been updated since 2015. It is
possible that there are dead links, or that the configuration syntax has changed
dramatically. These posts are also some of the most popular on my blog. I plan
to do a new guide eventually, but for right now please take the following
entries with a grain of salt.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Well I have finally persuaded myself to continue writing these posts by
completely deleting all the configuration I had already set up. It is worth
noting that I have switched over to Debian Jessie, for no other reason than
to cause myself more &lt;a href=&quot;http://jimlynch.com/linux-articles/the-psychology-of-a-distrohopper/&quot;&gt;frustration and suffering&lt;/a&gt;. Anyways, let’s get started.&lt;/p&gt;

&lt;p&gt;SNMP is considered an &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/agent-based-checks-addon&quot;&gt;Agent-Based Check&lt;/a&gt;, and is actually quite
flexible. You can even go as far as to code in custom return options, to check
things you normally wouldn’t be able to check over snmp, for example,
&lt;a href=&quot;https://wiki.icinga.org/display/howtos/check_apt+via+SNMP&quot;&gt;apt status&lt;/a&gt;, and other such things.&lt;/p&gt;

&lt;p&gt;It is worth noting that due to using a very small LAN, I will not be
fiddling around with SNMPv3, I will be going with straight SNMPv1,
just with a modified community string. We will get started with my core
router, Djehuti. It is outside the scope of this tutorial to discuss
how to enable SNMP on your device, but if you use a Ubiquiti device,
hey that might come soon.&lt;/p&gt;

&lt;p&gt;Starting from this post forward, I will be embedding code here instead of
referring to an external link, as embedding will encourage me to be a bit more
complete in my explanations. So, with all of that said, let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;initial-setup&quot;&gt;Initial Setup&lt;/h2&gt;
&lt;p&gt;To monitor SNMP we will be using the &lt;a href=&quot;http://nagios.manubulon.com/&quot;&gt;Manubulon SNMP Plugins&lt;/a&gt;. So we first
need to install them.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor:~$ sudo apt-get install nagios-snmp-plugins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to open up the main Icinga2 Configuration file and add in the
proper include to allow us to use these plugins. You may notice while poking
around this file that there are many things you either don’t need or would like
to change. I do plan to come back to this file at a later time, but feel free to
edit this file before that happens. Once you have made the proper changes,
restart Icinga2 so the new settings take effect.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor:~$ sudo vim /etc/icinga2/icinga2.conf

    include &amp;lt;manubulon&amp;gt;

zyradyl@captor:~$ sudo service icinga2 restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With that, we can move on to creating configuration files!&lt;/p&gt;

&lt;h2 id=&quot;djehuti&quot;&gt;Djehuti&lt;/h2&gt;
&lt;p&gt;We will be starting with my core router, which is running SNMPv1. The first
thing we will want to do is to add some essential variables to our host
directive so that we don’t have to redefine them with every service.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//
// Host Declaration Block
//
object Host &quot;djehuti.zyradyl.org&quot; {
    // Define the host IPv4 Address
    address             = &quot;10.0.0.1&quot;
    // Define a basic functionality test
    // Hostalive does a basic ICMP ECHO to the target
    // specified in the address directive.
    check_command       = &quot;hostalive&quot;
    // Define SNMP Variables
    vars.snmp_address   = &quot;10.0.0.1&quot;
    vars.snmp_community = &quot;zyradyl&quot;
    // These are not strictly needed. I add them
    // so I know at a glance what version of snmp
    // I am using.
    vars.snmp_v2        = &quot;false&quot;
    vars.snmp_v3        = &quot;false&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The new additions are any of the &lt;code class=&quot;highlighter-rouge&quot;&gt;var.snmp*&lt;/code&gt; commands located under the
&lt;code class=&quot;highlighter-rouge&quot;&gt;check_command&lt;/code&gt; line. With our host variables set up, we can now move on to
defining a service. The first service defined in the
&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/plugin-check-commands#snmp-manubulon-plugin-check-commands&quot;&gt;Icinga2 Manubulon Documentation&lt;/a&gt; is the &lt;code class=&quot;highlighter-rouge&quot;&gt;snmp-load&lt;/code&gt; check. Seems like a
good starting place to me!&lt;/p&gt;

&lt;h3 id=&quot;snmp-load&quot;&gt;SNMP-Load&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//    
// Service Declaration Block
// Service:     snmp_load
// Description: Uses SNMP commands to check the load averages
//              on the device.
//
object Service &quot;snmp-load&quot; {
    host_name           = &quot;djehuti.zyradyl.org&quot;
    // Set the type of load check to use.
    vars.snmp_load_type = &quot;netsl&quot;
    // Set the Load Average warning threshold.
    vars.snmp_warn      = &quot;5,3,2&quot;
    // Set the Load Average critical threshold.
    vars.snmp_crit      = &quot;6,5,3&quot;
    check_command       = &quot;snmp-load&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I feel I should take a minute to explain the warning and critical variables,
because the icinga2 documentation doesn’t do a very good job. When checking
load averages on *nix systems, there are three parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Average Load over one minute&lt;/li&gt;
  &lt;li&gt;Average Load over five Minutes&lt;/li&gt;
  &lt;li&gt;Average Load over fifteen minutes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since my router is a dual core device, I have set it up so that if the system
is at full load for 15 minutes, I get a warning. If it has one process over
full load for five minutes, I get a warning. If it is three processes over full
load in one minute, I want a warning. Same thing applies to critical. If you
are trying to figure out what to set your levels at, I tend to use the following
formulas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Warning:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1min: 2*(Number of Cores)+1&lt;/li&gt;
      &lt;li&gt;5min: (Number of Cores)+1&lt;/li&gt;
      &lt;li&gt;15min: (Number of Cores)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Critical:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1min: 3*(Number of Cores)&lt;/li&gt;
      &lt;li&gt;5min: 2*(Number of Cores)+1&lt;/li&gt;
      &lt;li&gt;15min: (Number of Cores)+1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you have your file saved, restart Icinga2, and check the web interface.
Your new check will likely have an &lt;em&gt;Unknown&lt;/em&gt; Status in purple, just click on
the check, and manually run it by clicking “Check Now” in the right most panel.&lt;/p&gt;

&lt;p&gt;With that, we can move on to the next check!&lt;/p&gt;

&lt;h3 id=&quot;snmp-memory&quot;&gt;SNMP-Memory&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//
// Service Declaration Block
// Service:     SNMP-Memory
// Description: Uses SNMP commands to check status of RAM
//              and swap on the device.
//
object Service &quot;snmp-memory&quot; {
    host_name      = &quot;djehuti.zyradyl.org&quot;
    // Set the Memory warning for Ram and swap Respectively.
    // Uses percents.
    vars.snmp_warn = &quot;50,0&quot;
    vars.snmp_crit = &quot;80,0&quot;
    check_command  = &quot;snmp-memory&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The warning and critical values are expressed as percentages of the total
amount of their applicable setting. The first one applies to RAM and the second
value corresponds to swap. Restart Icinga2 and log on to the web interface to
check that the new service works.&lt;/p&gt;

&lt;h3 id=&quot;snmp-storage&quot;&gt;SNMP-Storage&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//
// Service Declaration Block
// Service:     SNMP-Storage
// Description: Uses SNMP commands to check the status of disk
//              storage space.
//
object Service &quot;snmp-storage&quot; {
    host_name              = &quot;djehuti.zyradyl.org&quot;
    // Uses percents.
    vars.snmp_warn         = &quot;50&quot;
    vars.snmp_crit         = &quot;80&quot;
    // Specify which partition to monitor
    vars.snmp_storage_name = &quot;/root.dev&quot;
    check_command          = &quot;snmp-storage&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;snmp_storage_name&lt;/code&gt; variable is used to specify which device you want to
check the status of. If you aren’t sure which device you need to check, set
it to blank, then let it run. It will return a list of partitions that you can
check. Simply enter the name into that variable and you are good to go.&lt;/p&gt;

&lt;p&gt;Just as memory, &lt;code class=&quot;highlighter-rouge&quot;&gt;snmp-storage&lt;/code&gt; uses percent values in the warning and critical
threshold variables.&lt;/p&gt;

&lt;h3 id=&quot;snmp-interfaces&quot;&gt;SNMP-Interfaces&lt;/h3&gt;

&lt;p&gt;I personally like to specify a different service block for each interface
that I am monitoring, so I am not sure if it is possible to mix interfaces
together, but I don’t see any reason it wouldn’t be possible. I’m going to
list the interface configurations below, and if any variables need to be
explained I will do that below the code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//
// Service Declaration Block
// Service:     SNMP-Interface
// Description: Uses SNMP commands to check the status of
//              various network interfaces on device.
//
object Service &quot;snmp-int-lan&quot; {
    host_name                      = &quot;djehuti.zyradyl.org&quot;
    // Define interface variables.
    vars.snmp_interface            = &quot;eth0&quot;
    vars.snmp_interface_label      = &quot;LAN&quot;
    vars.snmp_interface_perf       = &quot;true&quot;
    vars.snmp_interface_bits_bytes = &quot;true&quot;
    vars.snmp_interface_megabytes  = &quot;true&quot;
    vars.snmp_interface_noregexp   = &quot;true&quot;
    vars.snmp_warncrit_percent     = &quot;true&quot;
    // Set warning and crits to 100 to disable.
    vars.snmp_warn                 = &quot;100,100&quot;
    vars.snmp_crit                 = &quot;100,100&quot;
    check_command                  = &quot;snmp-interface&quot;
}

//
// Service Declaration Block
// Service:     SNMP-Interface
// Description: Uses SNMP commands to check the status of
//              various network interfaces on device.
//
object Service &quot;snmp-int-wan&quot; {
    host_name                      = &quot;djehuti.zyradyl.org&quot;
    // Define interface variables.
    vars.snmp_interface            = &quot;eth1&quot;
    vars.snmp_interface_label      = &quot;WAN&quot;
    vars.snmp_interface_perf       = &quot;true&quot;
    vars.snmp_interface_bits_bytes = &quot;true&quot;
    vars.snmp_interface_megabytes  = &quot;true&quot;
    vars.snmp_interface_noregexp   = &quot;true&quot;
    vars.snmp_warncrit_percent     = &quot;true&quot;
    // Set warning and crits to 100 to disable.
    vars.snmp_warn                 = &quot;100,100&quot;
    vars.snmp_crit                 = &quot;100,100&quot;
    check_command                  = &quot;snmp-interface&quot;
}

//
// Service Declaration Block
// Service:     SNMP-Interface
// Description: Uses SNMP commands to check the status of
//              various network interfaces on device.
//
object Service &quot;snmp-int-dmz&quot; {
    host_name                      = &quot;djehuti.zyradyl.org&quot;
    // Define interface variables.
    vars.snmp_interface            = &quot;eth2&quot;
    vars.snmp_interface_label      = &quot;DMZ&quot;
    vars.snmp_interface_perf       = &quot;true&quot;
    vars.snmp_interface_bits_bytes = &quot;true&quot;
    vars.snmp_interface_megabytes  = &quot;true&quot;
    vars.snmp_interface_noregexp   = &quot;true&quot;
    vars.snmp_warncrit_percent     = &quot;true&quot;
    // Set warning and crits to 100 to disable.
    vars.snmp_warn                 = &quot;100,100&quot;
    vars.snmp_crit                 = &quot;100,100&quot;
    check_command                  = &quot;snmp-interface&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So a few things in here need some explanation. The variable
&lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_interface&lt;/code&gt; specifies which interface we will be checking.
&lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_interface_noregexp&lt;/code&gt; is related to this in that it tells icinga2
to not use regex matching. &lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_interface_label&lt;/code&gt; configures a label
that will be shown in the console. &lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_interface_megabytes&lt;/code&gt;, and
&lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_interface_bits_bytes&lt;/code&gt; tells Icinga2 that we want to see bandwidth
measured in megabits. These variables can be adjusted accordingly. Finally,
&lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_interface_perf&lt;/code&gt; tells Icinga2 that we want to monitor bandwidth
usage.&lt;/p&gt;

&lt;p&gt;As for warning and critical values, while I like to monitor my bandwidth, I
don’t actually care how high it goes, at least not at the moment. More relevant
than that is the fact that my bandwidth is much less than a gigabit, but let’s
move on from that. &lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_warncrit_percent&lt;/code&gt; says that we are going to
specify our warning and critical thresholds as a percent of total available
bandwidth on that port. I then set &lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_warn&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;vars.snmp_crit&lt;/code&gt; to
100 so that it is effectively disabled.&lt;/p&gt;

&lt;p&gt;Once activating these services, you should reset Icinga2. It is worth noting
that you will first get a pending, and then an unknown status for about five
minutes, depending on your check time. Icinga compares the newest reading to
a previous reading that is sufficently old enough, which is usually about five
minutes, to calculate what has changed. Until you have a row in the database
that is the proper age, you will get a big &lt;em&gt;Unknown&lt;/em&gt; status. Nothing to worry
about, check back in a half hour.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There is one more snmp check that is available, and that is the process check.
While I previously used this setup on my core router, it ended up causing some
rather wonky effects, so I have elected to not use it. This check would be
useful to monitor the status of a mission critical process, such as a webserver
or even a database server. It works by searching the process list for the number
of times a string appears, and then going from there. I may cover this in the
future, but I won’t be at the moment.&lt;/p&gt;

&lt;p&gt;Thank you for reading, and I hope to have part five up with less of a lag time.
I am also planning to do Icinga2 integration with slack soon, so stay tuned
for that!&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>HughesNet Gen4 and IPv6 PMTUD: A Tragedy</title>
   <link href="http://zyradyl.github.io/2015/08/22/HughesNet-And-PMTUD/"/>
   <updated>2015-08-22T14:50:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/22/HughesNet-And-PMTUD</id>
   <content type="html">&lt;p&gt;So before I went to bed last night I started experiencing some very odd issues
with my connection. I could connect to Skype, but I couldn’t visit twitter.
I could talk to Google, but not GitHub. I was able to ping my HT1100 gateway,
but my Icinga2 monitoring system reported a socket timeout of longer than 10
seconds on HTTP.&lt;/p&gt;

&lt;p&gt;I spent about three hours on it before I finally went to bed. I even went as
far as to spend five more dollars to purchase a 500MB token in order to see
if maybe I was being penalized for using too much data in my throttled state,
as I have been making use of aria2 to manage large downloads that would
otherwise suffer from a mysterious decryption failure when I was downloading
via HTTPS. Didn’t help.&lt;/p&gt;

&lt;p&gt;This morning I bit the bullet and changed my LAN’s MTU from 1280 to 1500.
You may be wondering why I refer to this as having to bite the bullet, since
an MTU of 1500 is standard. Well, come to find out that PMTUD is broken on
HughesNet. Something about what is done on the HughesNet side causes the
packets to become too large. Now, IPv6 is supposed to handle this by sending
ICMPv6 Type 2 “Packet too Large” notices to the end point. While debugging
PMTUD the other day (about a week ago) I set up my firewall so that &lt;strong&gt;ALL&lt;/strong&gt;
ICMPv6 is allowed, rather than having to itemize the different types. Still,
no good. I was getting silent failures that I had to use test-ipv6.com to
resolve, and they still indicated packets were becoming too large for my
connection. In a fit of irritation, I set my entire LAN to an MTU of 1280.
Eureka, I have IPv6 functionality.&lt;/p&gt;

&lt;p&gt;Obviously after last night, I can’t exactly use that solution anymore, for
a reason I have yet to understand. So my MTU is set back to 1500, and without
using HughesNet’s squid proxy (what they call web acceleration), IPv6 fails.
Oh, I didn’t mention that did I? Yeah, their web acceleration lets IPv6 work.
However, I run my own squid proxy, locally, which is even faster than theirs.
It also saves me bandwidth. So I keep web acceleration off.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Web Acceleration On = PMTUD works, IPv6 BUT I have double caching. Not good.&lt;/li&gt;
  &lt;li&gt;Web Acceleration Off = IPv6 Fails. Not acceptable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just to see if I could get something reset in the modem, I even called tech
support. The solution of “It is a problem with your LAN” was obvious, but still
frustrating due to the issues described above. However, I am planning to reach
out to their support today via social media, so I am pushing this article live,
without links, just so I have something to refer to. I will come back to link
to the major terms in this a little later. Wish me luck.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Just as an example..</title>
   <link href="http://zyradyl.github.io/2015/08/18/As-An-Example/"/>
   <updated>2015-08-18T23:15:45-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/18/As-An-Example</id>
   <content type="html">&lt;p&gt;of what I mean when I say I spend my time meddling in software or hardware
that I really don’t have any business meddling in, I’ve got this website
split into two branches now: Master, and Development. I do all my work in
development, then push to remote. Travis-CI then pulls the changes and builds
them, then runs HTML-Proofer over the output, and then sends me an e-mail
if something is broken. If nothing is broken, then I rebase my Development
branch on master, and then merge it into master to make sure my changes are
preserved.&lt;/p&gt;

&lt;p&gt;I guess it should read “I meddle in software that I don’t need in the
slightest.”&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Icinga2 Tutorial: Part 3 - Agent-Based Checks</title>
   <link href="http://zyradyl.github.io/2015/08/17/Icinga2-Tutorial-Part-3/"/>
   <updated>2015-08-17T00:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/17/Icinga2-Tutorial-Part-3</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/10/Icinga2-Tutorial-Part-0/&quot; title=&quot;Icinga2 Tutorial Part 0&quot;&gt;Icinga2 Tutorial: Part 0 - Network Monitoring for the Masses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/11/Icinga2-Tutorial-Part-1/&quot; title=&quot;Icinga2 Tutorial Part 1&quot;&gt;Icinga2 Tutorial: Part 1 - Installation and Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/12/Icinga2-Tutorial-Part-2/&quot; title=&quot;Icinga2 Tutorial Part 2&quot;&gt;Icinga2 Tutorial: Part 2 - Agent-Less Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/09/07/Icinga2-Tutorial-Part-4/&quot; title=&quot;Icinga2 Tutorial Part 4&quot;&gt;Icinga2 Tutorial: Part 4 - Extending Checks to SNMP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;EDIT (2018/12/09):&lt;/strong&gt; &lt;em&gt;These guides haven’t been updated since 2015. It is
possible that there are dead links, or that the configuration syntax has changed
dramatically. These posts are also some of the most popular on my blog. I plan
to do a new guide eventually, but for right now please take the following
entries with a grain of salt.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For this part of the tutorial, we are actually going to be rewriting the host
file for localhost. Unfortunately, I thought the EdgeRouter would be
capable of running an &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc&quot; title=&quot;Icinga2 Official Documentation&quot;&gt;Icinga2&lt;/a&gt; client, but it isn’t available through the
apt repositories, and after the &lt;code class=&quot;highlighter-rouge&quot;&gt;ntopng&lt;/code&gt; incident of July and August, I am
not stressing that processor any more than it needs to be to run my network.
(It is stressed enough as it is, but that is a post for another day.)&lt;/p&gt;

&lt;p&gt;Since I have done most of the explaining in the files, these tutorials will
start including the configuration files, and any external resources
I have used.&lt;/p&gt;

&lt;h3 id=&quot;configuration-file&quot;&gt;Configuration File&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/*
 * File:        /etc/icinga2/conf.d/captor.conf
 * Title:       Captor Host Configuration File
 * Description: Host and services definition for Icinga2 for
 *              the Captor Localhost.
 * Host:        captor.zyradyl.org
 * System:      Linux Mint Debian Edition 2
 * License:     Creative Commons Zero - Public Domain
 * Version:     1.0
 * Date:        August 16, 2015
 */

//
// Host Declaration Block
//
object Host &quot;captor.zyradyl.org&quot; {
    // Define the host IPv4 Address
    address         = &quot;127.0.0.1&quot;
    // Define the host IPv6 Address
    address6        = &quot;::1&quot;
    // Define the operating system
    vars.os         = &quot;Linux&quot;
    // Define a basic functionality test
    // Hostalive does a basic ICMP ECHO to the target
    // specified in the address directive.
    check_command   = &quot;hostalive&quot;
}

//
// There are two ways that we can configure Icinga2.
// The first method is to write specific files for
// every host, which is a good idea if you only have
// a few hosts, and every host has something
// different going on. The other method wold be to
// use apply service logic, which is better for
// larger deployments. I have provided links to
// both a best practices guide as well as to a guide
// on apply logic. I will be using files on a per
// host basis for my deployment.
//

// For demonstration purposes I am setting up an
// IPv6 Ping check.

//
// Service Declaration Block
// Service:     Ping6
// Description: Check if Host is responding to IPv6
// Note:        The address6 specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;ping6&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;ping6&quot;
}

//
// Service Declaration Block
// Service:     HTTP.server
// Description: Checks if Apache in general is up.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
// We are specifying an additional part on the service
// because we are also going to check to make sure that
// IcingaWeb2 is running.
//
object Service &quot;http.server&quot; {
    host_name               = &quot;captor.zyradyl.org&quot;
    // This tells Icinga to check only this specific
    // page.
    vars.http_uri           = &quot;/&quot;
    check_command           = &quot;http&quot;
}

//
// Service Declaration Block
// Service:     HTTP.icingaweb2
// Description: Check if the IcingaWeb service is up.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;http.icingaweb&quot; {
    host_name               = &quot;captor.zyradyl.org&quot;
    // This tells Icinga to check only this specific
    // page.
    vars.http_uri           = &quot;/icingaweb2&quot;
    check_command           = &quot;http&quot;
}

//
// Service Declaration Block
// Service:     SSL Cert Check
// Description: Check expiration date of SSL certificates.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
// The following is commented out because captor doesnt offer
// HTTPS.
//
//object Service &quot;ssl&quot; {
//    host_name                         = &quot;wepwawet.zyradyl.org&quot;
//    vars.ssl_port                     = &quot;443&quot;
//    vars.ssl_cert_valid_days_warn     = &quot;30&quot;
//    vars.ssl_cert_valid_days_critical = &quot;15&quot;
//    check_command                     = &quot;ssl&quot;
//}

//
// Service Declaration Block
// Service:     Disks
// Description: Checks status of all installed disks.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;disks&quot; {
    host_name                     = &quot;captor.zyradyl.org&quot;
    // We need to exclude a particular partition due to
    // access denials.
    vars.disk_partitions_excluded = &quot;/run/user/1000/gvfs&quot;
    check_command                 = &quot;disk&quot;
}

//
// Service Declaration Block
// Service:     Icinga
// Description: Checks status of Icinga Instance.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;icinga&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;icinga&quot;
}

// Service Declaration Block
// Service:     Load
// Description: Checks how much load the host is under.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;load&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;load&quot;
}

// Service Declaration Block
// Service:     Procs
// Description: Checks number of processes on host.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;procs&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;procs&quot;
}

//
// Service Declaration Block
// Service:     SSH
// Description: Checks if SSH is available.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
// Note:        Captor does not provide ssh services.
//              Commenting out this service.
//
//object Service &quot;ssh&quot; {
//    host_name     = &quot;captor.zyradyl.org&quot;
//    check_command = &quot;ssh&quot;
//}

// Service Declaration Block
// Service:     Swap
// Description: Checks status of swap space on host.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;swap&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;swap&quot;
}

// Service Declaration Block
// Service:     Users
// Description: Checks number of users on the system
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;users&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;users&quot;
}

// Service Declaration Block
// Service:     Apt
// Description: Checks status of the apt package manager.
// Note:        The address specified in the above directive  
//              is carried into the service declaration
//              blocks via specifying the host_name variable
//
object Service &quot;apt&quot; {
    host_name     = &quot;captor.zyradyl.org&quot;
    check_command = &quot;apt&quot;
}

// EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;import-notes&quot;&gt;Import Notes&lt;/h3&gt;
&lt;p&gt;It is worth noting that this article was intended to be a quick write up to
get something online. In time, I may come back to rewrite things, but at the
very least I hope the comments in the file are helpful.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Ubiquiti EdgeRouter Lite SquashFS Block Read Error: Part 1</title>
   <link href="http://zyradyl.github.io/2015/08/16/UBNT-ERL-SquashFS-Read-Error/"/>
   <updated>2015-08-16T00:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/16/UBNT-ERL-SquashFS-Read-Error</id>
   <content type="html">&lt;h2 id=&quot;problem-description&quot;&gt;Problem Description&lt;/h2&gt;
&lt;p&gt;Upon booting the EdgeRouter Lite, the system will not boot. Connecting to
console reveals the following log:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SQUASHFS error: zlib_inflate error, data probably corrupt
SQUASHFS error: squashfs_read_data failed to read block 0x1b4574
SQUASHFS error: Unable to read fragment cache entry [1b4574]
SQUASHFS error: Unable to read page, block 1b4574, size 8519
SQUASHFS error: Unable to read fragment cache entry [1b4574]
SQUASHFS error: Unable to read page, block 1b4574, size 8519
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This can be caused by an unclean shutdown, which could potentially occur with a
power failure. Due to the failure, parts of the internal ext3 filesystem
become corrupted, which causes the inability to load the SquashFS partition
which contains EdgeOS. The end result, of course, is a non-functional unit.&lt;/p&gt;

&lt;h2 id=&quot;problem-severity&quot;&gt;Problem Severity&lt;/h2&gt;
&lt;p&gt;There are four potential levels to determining the severity of this issue:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can the filesystem be salvaged by fscking the ext3 partition until it is
salvageable? (&lt;strong&gt;Severity:&lt;/strong&gt; Annoying.)&lt;/li&gt;
  &lt;li&gt;If the answer to the above is no, can the flash drive be saved by zeroing out
the device and writing a new filesystem to it? (&lt;strong&gt;Severity:&lt;/strong&gt; Hardware issue -
End user repair possible..)&lt;/li&gt;
  &lt;li&gt;If the answer to the above is no, is the device capable of booting? If yes, it
is possible you may be suffering
from the &lt;a href=&quot;https://community.ubnt.com/t5/EdgeMAX/EdgeRouter-LITE-OS-and-hardware-problems/td-p/667557&quot;&gt;EdgeRouter Lite RAM issue&lt;/a&gt;. (&lt;strong&gt;Severity:&lt;/strong&gt; Hardware issue -
RMA the device.)&lt;/li&gt;
  &lt;li&gt;If the answer to the above is no, is the device under warranty? If yes,
(&lt;strong&gt;Severity:&lt;/strong&gt; Hardware issue - RMA the device.) If no, (&lt;strong&gt;Severity:&lt;/strong&gt; Device
Replacement.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-resolution&quot;&gt;Problem Resolution&lt;/h2&gt;
&lt;p&gt;Resolutions will be broken down into two posts, one for each problem the user
could resolve without warranty work.&lt;/p&gt;

&lt;h3 id=&quot;resolution-credits&quot;&gt;Resolution Credits&lt;/h3&gt;
&lt;p&gt;A special thanks to Ubiquiti for providing a community support option via
their official website. Additionally, special thanks to all those users that
utilize said forum, your information was invaluable in helping me to fix my
EdgeRouter, and write this article.&lt;/p&gt;

&lt;h2 id=&quot;resolution-one-repair-the-file-system&quot;&gt;Resolution One: Repair the File System&lt;/h2&gt;
&lt;p&gt;This is by far the easiest resolution. Open the EdgeRouter Lite (&lt;strong&gt;Caution:&lt;/strong&gt;
&lt;em&gt;Your warranty is now void. Depending on how soon you need the device
back in operation, it may be acceptable for you to void the warranty on a
$100USD device if you can get it back in service by the end of the day. If
you think your device may need to be RMA’d, do not open the device and proceed
to UBNT’s Support Site.&lt;/em&gt;) by removing two screws on the bottom
of the device, located right under where the interface connections are. Your
device will slide apart, revealing the internal board. On the board you
will see a soldered on female USB port with a small silver USB flash drive,
roughly 3cm long. This is your 2GB of internal memory, but the device itself
is 2.6GB.&lt;/p&gt;

&lt;p&gt;Remove the USB drive and plug it into a USB port on your Mac OSX or Linux
powered device. Open a terminal, and run dmesg. You are looking for the
devfs name of your device.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[  262.645401] scsi 2:0:0:0: Direct-Access                               5.00 PQ: 0 ANSI: 2
[  262.647109] sd 2:0:0:0: Attached scsi generic sg1 type 0
[  262.648527] sd 2:0:0:0: [sdb] 4057088 512-byte logical blocks: (2.07 GB/1.93 GiB)
[  262.648795] sd 2:0:0:0: [sdb] Write Protect is off
[  262.648817] sd 2:0:0:0: [sdb] Mode Sense: 0b 00 00 08
[  262.649095] sd 2:0:0:0: [sdb] No Caching mode page found
[  262.649114] sd 2:0:0:0: [sdb] Assuming drive cache: write through
[  262.652309]  sdb: sdb1
[  262.655018] sd 2:0:0:0: [sdb] Attached SCSI removable disk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case, my device name is &lt;code class=&quot;highlighter-rouge&quot;&gt;sdb&lt;/code&gt;. Please note that for this guide I am not
using an actual EdgeRouter flash drive. This is a stand-in device. The next
thing to do is make sure none of the partitions on the device have been
automatically mounted. Where I used &lt;code class=&quot;highlighter-rouge&quot;&gt;sdb&lt;/code&gt; in the following example, use whatever
dmesg identified as your USB device.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ mount | grep sdb
/dev/sdb1 on /media/zyradyl/b7bcf200-26a1-41ed-9122-625558dbc907 type ext4 (rw,nosuid,nodev,relatime,data=ordered,uhelper=udisks2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This would indicate that at least one of the partitions on the disk has been
mounted. To correct this we need to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;umount&lt;/code&gt; command. Note that in
some operating systems an eject button exists in the taskbar. In my experience,
not only do these buttons unmount the mounted partitions, it also removed the
device entry from the kernel. So I prefer to use manual &lt;code class=&quot;highlighter-rouge&quot;&gt;umount&lt;/code&gt; commands. In
the following example, do one line per mounted partition, substituting the
device names of your partitions where I specified &lt;code class=&quot;highlighter-rouge&quot;&gt;/dev/sdb1&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ sudo umount /dev/sdb1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you have unmounted the devices, you can use fdisk to get a good view
of the partition layout on your USB device. I like to do this even if I know
what I should expect, just as a form of a sanity test to make sure I have the
right device. Note that, as previously stated, this device is a stand in. I have
tried to recreate the partition table to the best of my memory.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ sudo fdisk -l /dev/sdb

Disk /dev/sdb: 2 GiB, 2077229056 bytes, 4057088 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xd9dd6356

Device     Boot  Start     End Sectors  Size Id Type
/dev/sdb1         2048  264191  262144  128M  b W95 FAT32
/dev/sdb2       264192 4057087 3792896  1.8G 83 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So we can see from the above that we have two partitions on the USB device. In
your case, when using an actual EdgeRouter flash drive, one of the partitions
will be a vfat partition type, and the second one will be a linux
partition type. Now that we know where our partitions are, and that they are
safely unmounted, we can use fsck. Remember to replace my instance of &lt;code class=&quot;highlighter-rouge&quot;&gt;sdb2&lt;/code&gt;
with the correct partition for your device. Note that depending on the damage to
the filesystem, this has the potential to be a &lt;strong&gt;destructive operation&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ fsck.ext3 -fvy /dev/sdb2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Allow the program to run till completion. If you notice the command has become
looped, you will need to clear the journal out on your device. Note that
this is a &lt;strong&gt;potentially destructive operation&lt;/strong&gt; to data that had not been
written from the journal to the disk. The first thing that we need to do is
clear out the recovery indicator using &lt;code class=&quot;highlighter-rouge&quot;&gt;debugfs&lt;/code&gt; Remember to replace &lt;code class=&quot;highlighter-rouge&quot;&gt;sdb2&lt;/code&gt;
with the proper device name of your ext3 partition.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ debugfs -w -R “feature ^needs_recovery” /dev/sdb2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After clearing out this flag, it is now possible to use &lt;code class=&quot;highlighter-rouge&quot;&gt;tune2fs&lt;/code&gt; to force
removal of the journal.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ tune2fs -f -O ^has_journal /dev/sdb2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you can go back up and run the &lt;code class=&quot;highlighter-rouge&quot;&gt;fsck&lt;/code&gt; command. Once that is done, you will
need to re-enable the journal on your filesystem.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ tune2fs -f -O has_journal -j size=128M /dev/sdb2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you have a clean fsck output, and you have your journal enabled again, plug
the USB device back into the EdgeRouter and boot. If you are lucky, everything
will come back as it should. As there may be data loss, make sure to restore
from a recent configuration backup (you &lt;strong&gt;do&lt;/strong&gt; have configuration backups,
right?) and reboot, before logging into the device through ssh to make sure
everything is where it should be.&lt;/p&gt;

&lt;p&gt;If you are still having problems, stick around for part two of this guide, where
I will be walking you through using a separate host computer to recreate the
filesystem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT (2018/12/09):&lt;/strong&gt; &lt;em&gt;Part 2 was never written, but this is actually a fairly
common issue with EdgeRouter Lites. A simple Google search should turn up the
EdgeRouter Emergency Recovery Kit GitHub, and that plus some posts on the UBNT
forums should help you solve the problem. Sorry about my inability to deliver
on what I mention in my posts.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;external-links--resources&quot;&gt;External Links &amp;amp; Resources&lt;/h2&gt;
&lt;p&gt;[How to Recover an EXT3 Volume with an Unreadable Journal][2]&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Icinga2 Tutorial: Part 2 - Agent-Less Checks</title>
   <link href="http://zyradyl.github.io/2015/08/12/Icinga2-Tutorial-Part-2/"/>
   <updated>2015-08-12T03:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/12/Icinga2-Tutorial-Part-2</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/10/Icinga2-Tutorial-Part-0/&quot;&gt;Icinga2 Tutorial: Part 0 - Network Monitoring for the Masses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/11/Icinga2-Tutorial-Part-1/&quot;&gt;Icinga2 Tutorial: Part 1 - Installation and Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/17/Icinga2-Tutorial-Part-3/&quot;&gt;Icinga2 Tutorial: Part 3 - Agent-Based Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/09/07/Icinga2-Tutorial-Part-4/&quot;&gt;Icinga2 Tutorial: Part 4 - Extending Checks to SNMP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;EDIT (2018/12/09):&lt;/strong&gt; &lt;em&gt;These guides haven’t been updated since 2015. It is
possible that there are dead links, or that the configuration syntax has changed
dramatically. These posts are also some of the most popular on my blog. I plan
to do a new guide eventually, but for right now please take the following
entries with a grain of salt.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;master-host-configuration&quot;&gt;Master Host Configuration&lt;/h2&gt;
&lt;p&gt;So in our last part we focused on getting your machine set up as the
&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/distributed-monitoring#distributed-monitoring-setup-master&quot;&gt;Icinga2 master controller&lt;/a&gt;. Now we can focus on getting interoperability
setup. As always, this tutorial assumes you are sudo’d as root. You can do this
by running “sudo -s”. Then we need to set ourselves up as the master node on our
network.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # icinga2 node wizard
Welcome to the Icinga 2 Setup Wizard!

We'll guide you through all required configuration details.

Please specify if this is a satellite setup ('n' installs a master setup) [Y/n]: n
Starting the Master setup routine…
Please specifiy the common name (CN) [captor.zyradyl.org]:
information/cli: Generating new CSR in '/etc/icinga2/pki/captor.zyradyl.org.csr'.
information/cli: Created backup file '/etc/icinga2/pki/captor.zyradyl.org.key.orig'.
information/cli: Created backup file '/etc/icinga2/pki/captor.zyradyl.org.csr.orig'.
information/base: Writing private key to '/etc/icinga2/pki/captor.zyradyl.org.key'.
information/base: Writing certificate signing request to '/etc/icinga2/pki/captor.zyradyl.org.csr'.
information/cli: Signing CSR with CA and writing certificate to '/etc/icinga2/pki/captor.zyradyl.org.crt'.
information/cli: Created backup file '/etc/icinga2/pki/captor.zyradyl.org.crt.orig'.
information/cli: Copying CA certificate to '/etc/icinga2/pki/ca.crt'.
information/cli: Created backup file '/etc/icinga2/pki/ca.crt.orig'.
information/cli: Dumping config items to file '/etc/icinga2/zones.conf'.
Please specify the API bind host/port (optional):
Bind Host []:
Bind Port []:
information/cli: Enabling the APIlistener feature.
information/cli: Updating constants.conf.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
Done.
Now restart your Icinga 2 daemon to finish the installation!

captor zyradyl # service icinga2 restart
checking Icinga2 configuration.
[...]
captor zyradyl #
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that in your case, you may see one or two warning messages, but if it
pertains to files already existing, don’t worry about it, I also clipped all
the output that &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc&quot;&gt;Icinga2&lt;/a&gt; spits out when it restarts in order to save space.
These posts are long enough as it is. After restarting, if you check the
website for your master node, you will see a whole bunch of new information.
This has also established the certificates the &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/distributed-monitoring#distributed-monitoring-setup-satellite-client&quot;&gt;icinga2 protocol&lt;/a&gt; uses for
security.&lt;/p&gt;

&lt;h2 id=&quot;host-monitoring&quot;&gt;Host Monitoring&lt;/h2&gt;
&lt;p&gt;Now, while it is nice to have access to the Icinga protocol, in our case we
will be working with devices that do not make the option of installing Icinga
possible. Instead, we will be monitoring through four different
protocols: SSH, SNMP(v1), HTTP/S, and ICMP.&lt;/p&gt;

&lt;p&gt;We are going to go simple in this route through the use of
&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/monitoring-basics#check-commands&quot;&gt;agent-less checks.&lt;/a&gt; Agent-less checks do not rely on having a remote
program installed, and this is useful for embedded devices that may not
have enough memory to host another program.  This is also helpful if your
client only offers one or two services and it isn’t worth taking the time to
install and configure a node setup. For example, you can check if SSH is
available on a remote host, or check if the HTTP server is alive, or even
simply see if the host is alive. We will start there.&lt;/p&gt;

&lt;p&gt;Before we get into editing the actual files, Syntax highlighting can make life
a whole lot easier. Note, you should repeat this process with a non-privileged
user as that will give you a way to take a look at icinga2 configuration files
without having to be the root user.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor conf.d # mkdir -p ~/.vim/{syntax,ftdetect}
captor conf.d # cd /usr/share/icinga2-common/syntax/
captor syntax # cp vim/syntax/icinga2.vim ~/.vim/syntax/
captor syntax # cp vim/ftdetect/icinga2.vim ~/.vim/ftdetect/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now test it by opening any file under the &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/icinga2/conf.d/&lt;/code&gt; directory. You
can find instructions to do this for nano &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/getting-started#configuration-syntax-highlighting-nano&quot;&gt;here&lt;/a&gt;. Now, there are a lot of
ways to configure Icinga2. You can choose to use the pre-existing
&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/configuring-icinga2-first-steps#hosts-conf&quot;&gt;hosts.conf&lt;/a&gt; and &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/configuring-icinga2-first-steps#services-conf&quot;&gt;services.conf&lt;/a&gt;, or, since the &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/configuring-icinga2-first-steps#conf-d&quot;&gt;conf.d&lt;/a&gt; directory
is read on startup, you can use one file per host. I will be doing the latter,
as I think it keeps it a lot neater. The Access point is named Wepwawet, so
let’s get started. (Note that you clearly do not need to use my header method.
You can also indent with tabs, I mean.. if you like that kind of uncertainty in
how the file will be displayed.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor conf.d # vim /etc/icinga2/conf.d/wepwawet.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can find the document, with syntax highlighting, &lt;a href=&quot;http://pastebin.com/twTv3bem&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So with this basic configuration, we get three checks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An ICMP Echo (hostalive)&lt;/li&gt;
  &lt;li&gt;A HTTPS Up Check (http)&lt;/li&gt;
  &lt;li&gt;A SSH Up check (ssh)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For my HTTP check, the website uses HTTPS, and no HTTP site is available. So
by setting the SSL variable, we can ensure that just HTTPS is being checked.
This ensures we are getting an accurate reading. Now we need to ensure that our
declaration works.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor conf.d # /etc/init.d/icinga2 checkconfig
checking Icinga2 configuration.
captor conf.d # /etc/init.d/icinga2 reload
checking Icinga2 configuration.
Reloading icinga2 monitoring daemon: icinga2.
captor conf.d #
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then log in to your web page, and you should see your new host and service
definition. If this works, then move on to the next section. If not, you
should consult the error messages printed out as well as the Icinga2 log that
can be found in &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/log&lt;/code&gt;. A useful method to check if the error is in
your configuration is to run the checks manually. Attempt to ping the
host, or ssh to the host, or access a web page. If you are unable to do
these things manually, the problem may very well be with your host.&lt;/p&gt;

&lt;p&gt;The one final thing I would like to do is to demonstrate that you can actually
do a fairly nice amount of checks without needing to have a remote agent.
For this next example, we are going to setup a check that will verify if our
SSL certificate is still valid, or how long we have till it runs out.
Previously, this was done through an &lt;a href=&quot;http://namsep.blogspot.com/2014/07/icinga-2-https-and-ssl-key-expiry-check.html&quot;&gt;external custom command&lt;/a&gt; but Icinga2
now has a &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/plugin-check-commands#plugin-check-command-tcp&quot;&gt;TCP check plugin&lt;/a&gt; that can do this natively.&lt;/p&gt;

&lt;p&gt;Open your configuration file and make the edits. You can follow this file with
syntax colouring &lt;a href=&quot;http://pastebin.com/H7TMnCpQ&quot;&gt;as an example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once done, reload your configuration as demonstrated above. It is worth stating
that the reload parameter also calls checkconfig, however I prefer to run them
as two separate commands. It is also worth stating that you could combine the
SSL certificate check with the HTTPS check. I separate them because for me a
certificate expiring isn’t too big of a deal – it will only throw an error –
but the HTTPS server going down is a much bigger deal.&lt;/p&gt;

&lt;p&gt;You can see the
&lt;a href=&quot;http://pastebin.com/NnJCYuLM&quot;&gt;configuration of the monitoring setup for my core router here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;external-resources&quot;&gt;External Resources:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/monitoring-remote-systems#agent-less-checks&quot;&gt;Icinga2 Monitoring Remote Systems: Agentless Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/monitoring-basics#hosts-services&quot;&gt;Icinga2 Monitoring Basics: Hosts and Services&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/plugin-check-commands#plugin-check-command-ssh&quot;&gt;Icinga2 Plugin Check Commands: SSH&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/plugin-check-commands#plugin-check-command-ssl&quot;&gt;Icinga2 Plugin Check Commands: SSL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/monitoring-basics#command-passing-parameters&quot;&gt;Icinga2 Monitoring Basics: Command Passing Variables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc#!/icinga2/latest/doc/module/icinga2/chapter/cli-commands#config-validation&quot;&gt;Icinga2 Configuration Validation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Icinga2 Tutorial: Part 1 - Installation and Configuration</title>
   <link href="http://zyradyl.github.io/2015/08/11/Icinga2-Tutorial-Part-1/"/>
   <updated>2015-08-11T02:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/11/Icinga2-Tutorial-Part-1</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/10/Icinga2-Tutorial-Part-0/&quot;&gt;Icinga2 Tutorial: Part 0 - Network Monitoring for the Masses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/12/Icinga2-Tutorial-Part-2/&quot;&gt;Icinga2 Tutorial: Part 2 - Agent-less Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/17/Icinga2-Tutorial-Part-3/&quot;&gt;Icinga2 Tutorial: Part 3 - Agent-Based Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/09/07/Icinga2-Tutorial-Part-4/&quot;&gt;Icinga2 Tutorial: Part 4 - Extending Checks to SNMP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;EDIT (2018/12/09):&lt;/strong&gt; &lt;em&gt;These guides haven’t been updated since 2015. It is
possible that there are dead links, or that the configuration syntax has changed
dramatically. These posts are also some of the most popular on my blog. I plan
to do a new guide eventually, but for right now please take the following
entries with a grain of salt.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I wanted to get this out fairly quickly, because I just actually did this, and
while the default &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/getting-started#setting-up-icinga2&quot;&gt;Icinga2 tutorial&lt;/a&gt; is pretty good, it is lacking in some
areas, and since everything is fresh in my mind I wanted to go ahead and draft
this up.&lt;/p&gt;

&lt;p&gt;It is worth noting that in this post I will assume you are root. You can get to
root as a sustained environment by using &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo -s&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I do not condone this for day to day usage, it is &lt;strong&gt;bloody dangerous&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So, let’s get started. Sudo to root.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zyradyl@captor ~ $ sudo -s
[sudo] password for zyradyl:
captor zyradyl #
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;installing-icinga2&quot;&gt;Installing Icinga2&lt;/h3&gt;
&lt;p&gt;This assumes you are root, and on a Debian based system. For other
systems, you can find &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/getting-started#setting-up-icinga2&quot;&gt;additional documentation here&lt;/a&gt;. Note that I follow
that documentation very closely except for a few small notes, so it should be
easy to adjust this tutorial for your needs.&lt;/p&gt;

&lt;h4 id=&quot;adding-repositories&quot;&gt;Adding Repositories&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # wget -O - http://debmon.org/debmon/repo.key 2&amp;gt;/dev/null | apt-key add -
captor zyradyl # echo deb http://debmon.org/debmon debmon-jessie main &amp;gt;/etc/apt/sources.list.d/debmon.list
captor zyradyl # apt-get update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first command downloads the cryptographic key for the debmon
repository and hands it over to apt. Apt then installs that key, which allows
you to verify that you are using packages signed off by and validated by that
repository. The second command is then echoing the debmon main repository into
your apt-sources, so that the software available there is added to your
system’s list, which allows you to install with just a command. The third
command tells your system to update its internal package list to incorporate
the changes. After that we can go ahead and install.&lt;/p&gt;

&lt;h4 id=&quot;installation&quot;&gt;Installation&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # apt-get install icinga2 monitoring-plugins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will install both &lt;a href=&quot;https://www.icinga.org/icinga/icinga-2/&quot;&gt;icinga2&lt;/a&gt; and the nagios monitoring plugins that are
compatible. This gives you a very good base. Start the program through the
service command, and then set it to start on boot with &lt;code class=&quot;highlighter-rouge&quot;&gt;update-rc.d&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # service icinga2 start
captor zyradyl # update-rc.d icinga2 enable
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tada! You now have a data aggregator/network monitor setup! However, we
want a nice way to get information from our monitoring host, so now we need to
install the web front end.&lt;/p&gt;

&lt;h2 id=&quot;installing-icingaweb2&quot;&gt;Installing IcingaWeb2&lt;/h2&gt;
&lt;p&gt;Let’s start with all the essentials.&lt;/p&gt;

&lt;h3 id=&quot;package-installation&quot;&gt;Package Installation&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # apt-get install postgresql icinga2-ido-pgsql apache2 php5-ldap php5-imagick php5-pgsql php5-intl icingaweb2 icingaweb2-module-doc icingaweb2-module-monitoring icingaweb2-module-setup
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a big one, so if you are living on about a meg down, prepare to settle
in for a bit. In order, this will install the PostgreSQL database, the
&lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/advanced-topics#db-ido&quot;&gt;Icinga2 database connector&lt;/a&gt;, the apache2 web server, the ldap php5 module,
the imagemagick module for php5, the postgresql php5 module, the intl php5
module, the IcingaWeb2 core, the IcingaWeb2 documentation module, the IcingaWeb2
monitoring module, and the IcingaWeb2 setup module. Note that on debian systems,
it will prompt you to allow it to setup the database. I &lt;strong&gt;strongly&lt;/strong&gt; recommend
you say NO to that, and do it manually.&lt;/p&gt;

&lt;p&gt;Once this completes, get everything running and added to the default runlevel.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # service postgresql start
captor zyradyl # update-rc.d postgresql enable
captor zyradyl # service apache2 start
captor zyradyl # update-rc.d apache2 enable
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;database-configuration&quot;&gt;Database Configuration&lt;/h3&gt;
&lt;p&gt;Now we need to make that database. The first command defines a new user named
“&lt;em&gt;icinga&lt;/em&gt;” with a password of the same name. The second command then creates a
database named &lt;strong&gt;icinga&lt;/strong&gt;, and grants ownership to the user &lt;strong&gt;icinga&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor zyradyl # cd /tmp
captor tmp # sudo -u postgres psql -c &quot;CREATE ROLE icinga WITH LOGIN PASSWORD 'icinga';&quot;
captor tmp # sudo -u postgres createdb -O icinga -E UTF8 icinga
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once this is done, edit the file &lt;code class=&quot;highlighter-rouge&quot;&gt;pg_hba.conf&lt;/code&gt;. This file controls the way
that hosts can authenticate with your database. We want to make sure that
icinga can utilize standard password login, or md5.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # vim /etc/postgresql/9.4/main/pg_hba.conf

    # icinga
    local    icinga    icinga                    md5
    host     icinga    icinga    127.0.0.1/32    md5
    host     icinga    icinga    ::1/128         md5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Save, and then restart the database to activate the changes. Once restarted,
we can import the &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/chapter/getting-started#configuring-db-ido-postgresql&quot;&gt;IDO SQL&lt;/a&gt; schema into Postgres. The export line pushes
the password into the environment, thus allowing us to avoid having to type it
in.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # service postgresql restart
captor tmp # export PGPASSWORD=icinga
captor tmp # psql -U icinga -d icinga &amp;lt; /usr/share/icinga2-ido-pgsql/schema/pgsql.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to go ahead and enable the features that icinga2 will use.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # icinga2 feature enable command
captor tmp # icinga2 feature enable ido-pgsql
captor tmp # icinga2 feature enable livestatus
captor tmp # icinga2 feature enable statusdata
captor tmp # service icinga2 restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The final line restarts icinga2, which is the final step to enable these
modules. Now, we need to update the configuration file for ido-pgsql to connect
it to our database that we manually set up above.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # vim /etc/icinga2/features-enabled/ido-pgsql.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Fill in the needed information as appropriate. Now you need to update the
&lt;a href=&quot;http://php.net/manual/en/timezones.php&quot;&gt;date.timezone&lt;/a&gt; variable in the php configuration, which can be found
below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # vim /etc/php5/apache2/php.ini
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As an example, mine is set to “&lt;em&gt;America/Detroit&lt;/em&gt;”. Now, to allow the web server
to pass commands to icinga2, we need to modify the &lt;strong&gt;www-data&lt;/strong&gt; user into the
proper group. On Debian, this is the nagios group.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # usermod -a -G nagios www-data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we need to use icingacli to generate our authentication token to run
initial setup.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # icingacli setup token create
captor tmp # icingacli setup token show
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The second command will show you the token should you forget it. Now open a
browser and navigate to the localhost web page. and follow along.&lt;/p&gt;

&lt;p&gt;Something to note is that you should never select skip verification until you
have checked all your log files in &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/log&lt;/code&gt;, and you are certain everything
is correct, and the programming is just being buggy. Also keep in mind that
postgres uses &lt;strong&gt;port 5432&lt;/strong&gt;, not the default port of 3306. Finally, you should
set a password for the postgres’ database superuser, because you are
going to need to use the superuser’s account credentials in the setup program
to create the database.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captor tmp # sudo -u postgres psql postgres

# \password postgres
Enter Password:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will change the password, and with that, we can end this part. Next up
will be configuring hosts both capable of using the icinga2 protocol, and
configuring icinga to speak to simple hosts.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Icinga2 Tutorial: Part 0 - Network Monitoring for the Masses</title>
   <link href="http://zyradyl.github.io/2015/08/10/Icinga2-Tutorial-Part-0/"/>
   <updated>2015-08-10T01:00:00-05:00</updated>
   <id>http://zyradyl.github.io/2015/08/10/Icinga2-Tutorial-Part-0</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/11/Icinga2-Tutorial-Part-1/&quot;&gt;Icinga2 Tutorial: Part 1 - Installation and Configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/12/Icinga2-Tutorial-Part-2/&quot;&gt;Icinga2 Tutorial: Part 2 - Agent-Less Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/08/17/Icinga2-Tutorial-Part-3/&quot;&gt;Icinga2 Tutorial: Part 3 - Agent-Based Checks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2015/09/07/Icinga2-Tutorial-Part-4/&quot;&gt;Icinga2 Tutorial: Part 4 - Expanding Checks to SNMP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;EDIT (2018/12/09):&lt;/strong&gt; &lt;em&gt;These guides haven’t been updated since 2015. It is
possible that there are dead links, or that the configuration syntax has changed
dramatically. These posts are also some of the most popular on my blog. I plan
to do a new guide eventually, but for right now please take the following
entries with a grain of salt.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This will be a bit more informal than most of my posts as this is more of a
hobby project than anything with really standardized applications. I’ve been
exploring the network protocols available to me on my EdgeRouter now for the
past year, and last night I sat down and taught myself SNMP. After three
hours and poking around the MIBs, I realized that I absolutely hate SNMP.
That being said, I very much like the idea of it. I have to use a CLI and
a GUI on the same device most of the time to watch all the stats I like to
watch, and I immensely dislike putting that kind of rendering work on my
EdgeRouter, because I can assure you, the poor thing is taxed enough.&lt;/p&gt;

&lt;p&gt;So I started exploring multi-protocol management systems today. Clearly,
Spiceworks is one of the best programs you can get. However, it is Windows
only. Here on Linux, I really only have NAGIOS, or so I thought.&lt;/p&gt;

&lt;h2 id=&quot;enter-icinga2&quot;&gt;Enter Icinga2&lt;/h2&gt;
&lt;p&gt;Icinga2 is a very interesting program to me in the same way that Maxthon
was a very interesting browser. Both Maxthon 2 and Icinga 1 were built on top
of existing applications (Internet Explorer and NAGIOS, respectively) with the
intent of expanding their functionality. I know a good many people who would
insist that NAGIOS has all the functionality you could ever need, and I would
agree, except for one thing. If I want ease of configuration, I have to pay
&lt;a href=&quot;https://assets.nagios.com/handouts/nagiosxi/Nagios-XI-2014-Pricing-Documentation.pdf&quot;&gt;$2000USD&lt;/a&gt; for my small home network. I don’t have that kind of money, and
if I did, you can bet your ass it would go to getting me embedded development
boards or external controllers for debugging and programming. Anyways, then
Maxthon 3 came out, and it stepped away from the Trident engine, and went with
Google’s WebKit. Just like Maxthon, Icinga2 has stepped away from NAGIOS and has
become its own solution, but still remains compatibility with NAGIOS monitoring
plugins.&lt;/p&gt;

&lt;h2 id=&quot;purpose-of-these-posts&quot;&gt;Purpose of These Posts&lt;/h2&gt;
&lt;p&gt;At first this was just going to be something fun for me to do. However, I have
noticed a somewhat disturbing lack of end to end documentation for Icinga.
Yes, all the information is there in the &lt;a href=&quot;http://docs.icinga.org/icinga2/latest/doc/module/icinga2/toc&quot;&gt;manuals&lt;/a&gt;, but it is still nice to
see how someone sets it up from end to end. This series of posts is going to
show how to do just that, starting with how to install Icinga, which will go up
in the next post.&lt;/p&gt;

&lt;h2 id=&quot;some-things-to-consider&quot;&gt;Some Things to Consider&lt;/h2&gt;
&lt;p&gt;This is my first time using Icinga2. You will be learning right along with me.
Which means some posts may spend a lot of time going back and working on things
that should have already been done. Besides that, I am not using an Icinga
dedicated machine, which means I am running a desktop release, Linux Mint
Debian Edition 2 to be exact. I like 2. It is a good number. Also the
distribution is really nice, and DOESN’T have systemd, but
that’s a post for another day.&lt;/p&gt;

&lt;p&gt;Additionally, my deployment is rather small. All told I will be monitoring
about 10 devices at most across four or five protocols, a fairly standard home
network. I will keep adding updates the more I learn about it, or if I add more
services. However, you may need to do some extra research to tweak things for
your deployment.&lt;/p&gt;

&lt;p&gt;I do plan to branch off into Netflow at some point. So you can expect
that in the future.&lt;/p&gt;

</content>
 </entry>
 

</feed>
